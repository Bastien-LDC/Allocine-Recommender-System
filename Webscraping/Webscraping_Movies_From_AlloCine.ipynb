{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎦**Web Scraping Movies Data From AlloCiné.fr**🎬\n",
    "\n",
    "This script builds a DataFrame by web scraping the **movies** data from AlloCiné — a company which provides information on French cinema. Because of the long delay, we choose to scrape the data in two steps : \n",
    "- First, we scrape the url of each movie with `getMoviesUrl(...)`\n",
    "- Lastly, we use the url list to scrape the data for each movie with `ScrapeURL(...)`\n",
    "\n",
    "*📝Note 1: We use the popular BeautifulSoup package*\n",
    "\n",
    "## **Functions :**\n",
    "\n",
    "### `getMoviesUrl(start_page, end_page, nb_pages)` :\n",
    "\n",
    "Returns the list of movies urls and a list of the movies that have not been released yet. It saves the list of movies urls in a CSV file as `../Movies/Data/movie_url.csv`. The argument must be integers and are used to select the range of pages (or the number of pages, default 1) you want to scrape the data from. The `end_page` is included. If the `end_page` value is correct (not `None` or >= `start_page`), the `nb_pages` argument is ignored. We only keep the urls of the movies which **have already been released**.\n",
    "\n",
    "### `ScrapeURL(movie_url, dwld_poster)` :\n",
    "\n",
    "Iterate over the list of urls generated by `getMoviesUrl(...)` and scrape the data for each movie. In the process, we extract :\n",
    "\n",
    "- `id`: AlloCiné movie id\n",
    "- `title`: the movie title (in French)\n",
    "- `release_date`: the movie release date\n",
    "- `duration`: the movie length (in minutes)\n",
    "- `genres`: the movie genres (as a CSV string)\n",
    "- `directors`: movie directors (as a CSV string)\n",
    "- `actors`: main movie actors (as a CSV string)\n",
    "- `nationality`: nationality of the movie (as a CSV string)\n",
    "- `press_rating`: average press rating (from 0.5 to 5 stars ⭐⭐⭐⭐⭐)\n",
    "- `nb_press_rating`: number of ratings made by the press\n",
    "- `spect_rating`: average AlloCiné users rating (from 0.5 to 5 stars ⭐⭐⭐⭐⭐)\n",
    "- `nb_spect_rating`: number of ratings made by the users/spectators\n",
    "- `summary`: the movie summary (in French)\n",
    "- `poster_link`: url of the movie poster\n",
    "\n",
    "*📝Note 2: We can choose to download the poster image with the `dwld_poster` argument. If `True`, the poster image is downloaded and saved in the `../Movies/Posters/` folder.*\n",
    "\n",
    "*📝Note 3: Intermediate functions were created to retrieve each individual feature of the dataframe, which makes it easier for debugging.*\n",
    "\n",
    "The function `ScrapeURL(...)` returns two objects : the data as a dataframe and the url list of errors as a list. In addition the two objects are saved as `../Movies/Data/allocine_movies.csv` and `../Movies/Data/allocine_errors.csv`. Until now, the remaining errors were passed into ScrapeURL(...) for debugging and understanding what went wrong. The generated error messages were used to correct the functions in which the error occured. The objective is to retrieve the maximum number of movies in the first place, before proceeding to a data cleaning.\n",
    "\n",
    "(⚠️ **Warning** ⚠️: the process can take a while, depending on the number of pages you choose to scrape. It is recommended to use a dedicated computer for this process, as it can take a long time to complete.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## **Import libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from time import time, sleep\n",
    "from datetime import timedelta, datetime, date\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import dateparser\n",
    "import os\n",
    "\n",
    "from warnings import warn, filterwarnings\n",
    "from IPython.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Functions:** Getting movie infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie ID: `get_movie_ID(movie_url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_ID(movie_url: str) -> int:\n",
    "    '''\n",
    "    Get the movie ID from the movie page url.\n",
    "    :param movie_url: URL of the movie page.\n",
    "    :return: The movie ID.'''\n",
    "    movie_ID = int(re.sub(r\"\\D\", \"\", movie_url))\n",
    "    return movie_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie title: `get_movie_title(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_title(movie_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the movie title from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie title.'''\n",
    "    movie_title = movie_soup.find(\"div\", {\"class\": \"titlebar-title\"}).text.strip()\n",
    "    return movie_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie release date: `get_movie_release_date(movie_soup)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert French months to English months: `convert_month(month)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_month(month: str) -> str:\n",
    "    '''\n",
    "    Convert French months into English months for the dateparser to work.\n",
    "    :param month: French month.\n",
    "    :return: English month.'''\n",
    "    if month == \"janvier\":\n",
    "        return \"January\"\n",
    "    elif month == \"février\":\n",
    "        return \"February\"\n",
    "    elif month == \"mars\":\n",
    "        return \"March\"\n",
    "    elif month == \"avril\":\n",
    "        return \"April\"\n",
    "    elif month == \"mai\":\n",
    "        return \"May\"\n",
    "    elif month == \"juin\":\n",
    "        return \"June\"\n",
    "    elif month == \"juillet\":\n",
    "        return \"July\"\n",
    "    elif month == \"août\":\n",
    "        return \"August\"\n",
    "    elif month == \"septembre\":\n",
    "        return \"September\"\n",
    "    elif month == \"octobre\":\n",
    "        return \"October\"\n",
    "    elif month == \"novembre\":\n",
    "        return \"November\"\n",
    "    elif month == \"décembre\":\n",
    "        return \"December\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function: `get_movie_release_date(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_release_date(movie_soup: BeautifulSoup) -> date:\n",
    "    '''\n",
    "    Scrape the movie release date from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie release date.'''\n",
    "    movie_release_date = movie_soup.find(\"span\", {\"class\": \"date\"})\n",
    "    if movie_release_date:\n",
    "        try:\n",
    "            movie_release_date = movie_release_date.text.strip()\n",
    "            month = movie_release_date.split(' ')[1]\n",
    "            movie_release_date = movie_release_date.replace(month, convert_month(month))\n",
    "            movie_release_date = dateparser.parse(movie_release_date, date_formats=[\"%d %B %Y\"]).date()      \n",
    "        except:\n",
    "            warn(f'The date is not in the expected format! The movie is skipped.')\n",
    "            return None  # If the date is not in the expected format, return None\n",
    "    return movie_release_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie duration: `get_movie_duration(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_duration(movie_soup: BeautifulSoup) -> int:\n",
    "    '''\n",
    "    Scrape the movie duration from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie duration in minutes if exists. Else None.'''\n",
    "    movie_duration_str = movie_soup.find(\"span\", {\"class\": \"spacer\"}).next_sibling.strip()\n",
    "    movie_duration = None\n",
    "    if movie_duration_str != \"\":\n",
    "        duration_timedelta = pd.to_timedelta(movie_duration_str).components\n",
    "        movie_duration = int(duration_timedelta.hours * 60 + duration_timedelta.minutes)\n",
    "    return movie_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie genres: `get_movie_genres(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_genres(movie_soup: BeautifulSoup) -> str:\n",
    "    ''' \n",
    "    Scrape the movie genres from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie genres as a CVS string. None if no genres are found.'''\n",
    "    div_genres = movie_soup.find(\"div\", {\"class\": \"meta-body-item meta-body-info\"})\n",
    "    if div_genres:\n",
    "        movie_genres = [\n",
    "            genre.text\n",
    "            for genre in div_genres.find_all(\"span\", class_=re.compile(r\".*==$\"))\n",
    "            if \"\\n\" not in genre.text\n",
    "        ]\n",
    "        return \", \".join(movie_genres)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie directors: `get_movie_directors(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_directors(movie_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the movie directors from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie directors as a CSV string. None if no directors are found.'''\n",
    "    div_directors = movie_soup.find_all(\n",
    "            \"div\", {\"class\": \"meta-body-item meta-body-direction\"}\n",
    "        )\n",
    "    # We retrieve the people next to the \"Par\" and \"De\" keywords, \n",
    "    # and we keep only one instance of each person.\n",
    "    if div_directors:\n",
    "        movie_directors = [\n",
    "            link.text\n",
    "            for directors in div_directors\n",
    "            for link in directors.find_all(\n",
    "                [\"a\", \"span\"], class_=re.compile(r\".*blue-link$\")\n",
    "            )\n",
    "        ]\n",
    "        return \", \".join(set(movie_directors))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie actors: `get_movie_actors(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_actors(movie_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the movie actors from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie actors as a CSV string. None if no actors are found.'''\n",
    "    div_actors = movie_soup.find(\"div\", {\"class\": \"meta-body-item meta-body-actor\"})\n",
    "    if div_actors:\n",
    "        # We retrieve the people next to the \"Avec\" keyword\n",
    "        movie_actors = [actor.text for actor in div_actors.find_all([\"a\", \"span\"])][1:]\n",
    "        return \", \".join(movie_actors)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie nationality: `get_movie_nationality(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_nationality(movie_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the movie nationality from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie nationality as a CSV string.'''\n",
    "    movie_nationality = [\n",
    "            nationality.text.strip()\n",
    "            for nationality in movie_soup.find_all(\"span\", class_=\"nationality\")\n",
    "        ]\n",
    "    return \", \".join(movie_nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie press ratings: `get_movie_press_rating(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_press_rating(movie_soup: BeautifulSoup) -> float:\n",
    "    '''\n",
    "    Scrape the movie average press rating from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie average press rating. None if no rating is found.'''\n",
    "    movie_ratings = movie_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in movie_ratings:\n",
    "        if \"Presse\" in ratings.text:\n",
    "            # eg: Change from 4,5 to 4.5.\n",
    "            return float(\n",
    "                re.sub(\n",
    "                    \",\", \".\", ratings.find(\"span\", {\"class\": \"stareval-note\"}).text\n",
    "                )\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie number of press ratings: `get_movie_press_rating_count(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_press_rating_count(movie_soup: BeautifulSoup) -> int:\n",
    "    '''\n",
    "    Scrape the movie number of press ratings from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie number of press ratings. None if no rating is found.'''\n",
    "    movie_ratings = movie_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in movie_ratings:\n",
    "        if \"Presse\" in ratings.text:\n",
    "            # We keep only the number of ratings, and we leave the number of reviews out.\n",
    "            # (eg: re.match(\"\\s\\d+\", \" 10154 notes dont 1327 critiques\").group() returns \" 10154\")\n",
    "            return int(\n",
    "                re.match(\n",
    "                    r\"\\s\\d+\",\n",
    "                    ratings.find(\"span\", {\"class\": \"stareval-review\"}).text,\n",
    "                ).group()\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie spectator ratings: `get_movie_spect_rating(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_spect_rating(movie_soup: BeautifulSoup) -> float:\n",
    "    '''\n",
    "    Scrape the movie average spectators' rating from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie average spec rating. None if no rating is found.'''\n",
    "    # get all the available ratings\n",
    "    movie_ratings = movie_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in movie_ratings:\n",
    "        if \"Spectateurs\" in ratings.text:\n",
    "            # eg: Change from 4,5 to 4.5.\n",
    "            return float(\n",
    "                re.sub(\n",
    "                    \",\", \".\", ratings.find(\"span\", {\"class\": \"stareval-note\"}).text\n",
    "                )\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie number of spec ratings: `get_movie_spect_rating_count(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_spect_rating_count(movie_soup: BeautifulSoup) -> int:\n",
    "    '''\n",
    "    Scrape the movie number of spec ratings from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie number of spec ratings. None if no rating is found.'''\n",
    "    movie_ratings = movie_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in movie_ratings:\n",
    "        if \"Spectateurs\" in ratings.text:\n",
    "            # We keep only the number of ratings, and we leave the number of reviews out.\n",
    "            # (eg: re.match(\"\\s\\d+\", \" 10154 notes dont 1327 critiques\").group() returns \" 10154\")\n",
    "            return int(\n",
    "                re.match(\n",
    "                    r\"\\s\\d+\",\n",
    "                    ratings.find(\"span\", {\"class\": \"stareval-review\"}).text\n",
    "                ).group()  \n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie summary: `get_movie_summary(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_summary(movie_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the movie summary from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie summary. None if no summary is found.'''\n",
    "    movie_summary = movie_soup.find(\"section\", {\"class\": \"section ovw ovw-synopsis\"})\n",
    "    # Check if the synopsis division exists \n",
    "    if movie_summary:\n",
    "        movie_summary = movie_summary.find(\"div\", {\"class\": \"content-txt\"})\n",
    "        # Check if the synopsis text exists\n",
    "        if movie_summary:\n",
    "            movie_summary = movie_summary.text.strip()\n",
    "            return unicodedata.normalize(\"NFKC\", movie_summary)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get movie poster: `get_movie_poster(movie_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_poster(movie_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the movie poster link from the movie page.\n",
    "    :param movie_soup: BeautifulSoup object of the movie page.\n",
    "    :return: The movie poster link.'''\n",
    "    return movie_soup.find(\"img\", {\"class\": \"thumbnail-img\"})[\"src\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download movie poster: `download_movie_poster(poster_link, movie_title, movie_id)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_movie_poster(poster_link: str, movie_title: str, movie_id: int=None) -> None:\n",
    "    '''\n",
    "    Download the movie poster from the poster link.\n",
    "    :param poster_link: The poster link.\n",
    "    :param movie_title: The movie title.\n",
    "    :param movie_id: The movie id.\n",
    "    :return: nothing but saves the movie poster as a jpg file.'''\n",
    "    poster = get_url(poster_link)\n",
    "    poster_path = \"../Movies/Posters/\"\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(poster_path), exist_ok=True) \n",
    "    movie_title = re.sub(r\"( |:|,|'|\\?|\\\\|/|\\*|<|>|\\\")\", \"_\", movie_title)\n",
    "    save_path = f\"{poster_path}{movie_title}_poster_{movie_id}.jpg\"\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(poster.read())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download all movie posters: `download_all_posters(movies_df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_posters(movies_df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Download all the movie posters from the poster links in the movies dataframe.\n",
    "    :param movies_df: The movies dataframe.\n",
    "    :return: nothing but saves the movie posters as jpg files.'''\n",
    "    for index, row in movies_df.iterrows():\n",
    "        download_movie_poster(row[\"poster_link\"], row[\"title\"], row[\"id\"])\n",
    "        sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## **Function:** `getMoviesUrl(start_page, end_page, nb_pages)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get url: `get_url(url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url: str):\n",
    "    '''\n",
    "    Try to get and open the page url.\n",
    "    :param url: url to get the content from.\n",
    "    :return: the content of the url page\n",
    "    '''\n",
    "    page = ''\n",
    "    while page == '':\n",
    "        try:\n",
    "            page = urlopen(url)\n",
    "            break\n",
    "        except:\n",
    "            print(f\"Error occured when opening the url.\\nURL: {url}\")\n",
    "            print(\"\\nConnection refused by the server...\")\n",
    "            print(\"Let's wait for 5 seconds\")\n",
    "            print(\"ZZzzzz...\")\n",
    "            sleep(5)\n",
    "            print(\"Ok, now let's try again...\\n\")\n",
    "            continue\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function: `getMoviesUrl(start_page, end_page, nb_pages)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def getMoviesUrl(start_page: int, end_page: int=None, nb_pages: int=1) -> None:\n",
    "    '''\n",
    "    Scrape the movies urls from the AlloCine website's movie page (http://www.allocine.fr/films/).\n",
    "    The range of pages to scrape goes from start_page to end_page (included) if end_page is not None or <= start_page.\n",
    "    Else, the range of pages to scrape goes from start_page to start_page + nb_pages.\n",
    "    It will ignore the movies that has not been released yet.\n",
    "    :param start_page: The first page to scrape.\n",
    "    :param end_page: The last page to scrape (included) (optional).\n",
    "    :param nb_pages: The number of pages to scrape (default 1).\n",
    "    :return: List of movies_url and a list of the movies that have not been released yet.\n",
    "    Saves the list of movies urls in a csv file.'''\n",
    "    if start_page <= 0:\n",
    "        return ValueError('start_page must be positive !')       \n",
    "\n",
    "    # We ignore dateparse warnings\n",
    "    filterwarnings(\"ignore\",message=\"The localize method is no longer necessary, as this time zone supports the fold attribute\")\n",
    "    \n",
    "    # Set the list\n",
    "    movie_url = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = start_page\n",
    "    # We will scrape by default at least 1 page if end_page is not specified, \n",
    "    # or if it is lower than start_page,\n",
    "    # or if the number of pages to scrape is negative.\n",
    "    if nb_pages < 1:\n",
    "        nb_pages = 1\n",
    "    if end_page == None or end_page < start_page:\n",
    "        end_page = start_page + nb_pages - 1\n",
    "    \n",
    "    # Number of movie requests\n",
    "    m_requests = 0\n",
    "    # Store the series not yet released \n",
    "    nr_movies = []\n",
    "    \n",
    "    # Loop over the pages\n",
    "    # Monitoring the loop with tqdm\n",
    "    for p in tqdm(range(start_page, end_page + 1), desc=\"Fetching movies urls\", unit=\"pages\"):\n",
    "\n",
    "        # Get request\n",
    "        url = f'http://www.allocine.fr/films/?page={p}'\n",
    "        response = get_url(url)\n",
    "        \n",
    "        # Pause the loop\n",
    "        sleep(randint(1,2))\n",
    "            \n",
    "        # Monitoring the requests\n",
    "        elapsed_time = time() - start_time\n",
    "        #print(f'>Page Request: {p_requests}; Frequency: {p_requests/elapsed_time} requests/s')\n",
    "            \n",
    "        # Warning for non-200 status codes\n",
    "        if response.status != 200:\n",
    "            warn(f'>Page Request: {p_requests}; Status code: {response.status_code}')\n",
    "\n",
    "        # Break the loop if the number of requests is greater than expected\n",
    "        if p_requests > end_page:\n",
    "            warn('Number of requests was greater than expected.')\n",
    "            break\n",
    "\n",
    "        # Parse the content of the request with BeautifulSoup\n",
    "        html_text = response.read().decode(\"utf-8\")\n",
    "        html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "        # Select all the movies url from a single page\n",
    "        movies = html_soup.find_all('h2', 'meta-title')\n",
    "        m_requests += len(movies)\n",
    "               \n",
    "        # Monitoring the requests\n",
    "        #print(f'>Page Request: {p_requests}; Movie Request: {m_requests}')\n",
    "        \n",
    "        # Pause the loop\n",
    "        sleep(1)\n",
    "        \n",
    "        for movie in movies:\n",
    "            m_url = f'http://www.allocine.fr{movie.a[\"href\"]}'\n",
    "            m_soup = BeautifulSoup(get_url(m_url), 'html.parser')\n",
    "            m_rd = get_movie_release_date(m_soup)\n",
    "            # We keep the movie url only if the movie has already been released.\n",
    "            if m_rd != None and m_rd <= datetime.now().date():\n",
    "                movie_url.append(m_url) \n",
    "            else:\n",
    "                nr_movies.append(m_url)\n",
    "        p_requests += 1\n",
    "        print(f\"{len(movie_url)}/{m_requests} URLs collected.\")\n",
    "        sleep(.1)\n",
    "\n",
    "    # Saving the files\n",
    "    movie_path = '../Movies/Data/'\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(movie_path), exist_ok=True) \n",
    "    print(f'--> Done; {p_requests-start_page} Page Requests and {m_requests-len(nr_movies)}/{m_requests} Movie Requests in {timedelta(seconds=time()-start_time)}\\n--> {len(nr_movies)} movies not yet released.')\n",
    "    r = np.asarray(movie_url)\n",
    "    np.savetxt(f\"{movie_path}movie_url.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    return movie_url, nr_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Function:** `ScrapeURL(movie_url, dwld_poster)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ScrapeURL(movie_url: list, dwld_poster: bool = False) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Scrape the data from the movies page urls.\n",
    "    :param movie_url: The list of movies page urls.\n",
    "    :param dwld_poster: Boolean to download the movie poster (default False).\n",
    "    :return: The dataframe of the movies and the dataframe of the urls that return an error. \n",
    "    Both are saved into csv files.'''\n",
    "    # init the dataframe\n",
    "    c = [\"id\",\n",
    "        \"title\",\n",
    "        \"release_date\",\n",
    "        \"duration\",\n",
    "        \"genres\",\n",
    "        \"directors\",\n",
    "        \"actors\",\n",
    "        \"nationality\",\n",
    "        \"press_rating\",\n",
    "        \"nb_press_rating\",\n",
    "        \"spect_rating\",\n",
    "        \"nb_spect_rating\",\n",
    "        \"summary\",\n",
    "        \"poster_link\"\n",
    "    ]\n",
    "    df = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    n_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors = []\n",
    "    \n",
    "    # request loop\n",
    "    for url in tqdm(movie_url, desc = \"Scraping movies data\"):\n",
    "        try :\n",
    "            response = get_url(url)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Monitoring the requests\n",
    "            n_request += 1\n",
    "            print\n",
    "            \n",
    "            elapsed_time = time() - start_time\n",
    "            #print(f'Request #{n_request}; Frequency: {n_request/elapsed_time} requests/s')\n",
    "            #clear_output(wait = True)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Warning for non-200 status codes\n",
    "            if response.status != 200:\n",
    "                warn(f'Request #{n_request}; Status code: {response.status_code}')\n",
    "                errors.append(url)\n",
    "\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            html_text = response.read().decode(\"utf-8\")\n",
    "            movie_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "            \n",
    "            # Get the movie data\n",
    "            if movie_html_soup.find('div', 'titlebar-title'):\n",
    "                # Get the movie ID \n",
    "                tp_id = get_movie_ID(url)\n",
    "                # Scrape the title\n",
    "                tp_title = get_movie_title(movie_html_soup)\n",
    "                # Scrape the release date\n",
    "                tp_release_dt = get_movie_release_date(movie_html_soup)\n",
    "                # Scrape the duration\n",
    "                tp_duration = get_movie_duration(movie_html_soup)\n",
    "                # Scrape the genres\n",
    "                tp_genre = get_movie_genres(movie_html_soup)\n",
    "                # Scrape the directors\n",
    "                tp_director = get_movie_directors(movie_html_soup)\n",
    "                # Scrape the actors\n",
    "                tp_actor = get_movie_actors(movie_html_soup)\n",
    "                # Scrape the nationality\n",
    "                tp_nation = get_movie_nationality(movie_html_soup)\n",
    "                # Scrape the press ratings\n",
    "                tp_press_rating = get_movie_press_rating(movie_html_soup)\n",
    "                # Scrape the number of press ratings\n",
    "                tp_nb_press_rating = get_movie_press_rating_count(movie_html_soup)\n",
    "                # Scrape the spec ratings\n",
    "                tp_spec_rating = get_movie_spect_rating(movie_html_soup)\n",
    "                # Scrape the number of spec ratings\n",
    "                tp_nb_spec_rating = get_movie_spect_rating_count(movie_html_soup)\n",
    "                # Scrape the summary\n",
    "                tp_summary = get_movie_summary(movie_html_soup)\n",
    "                # Scrape the poster\n",
    "                tp_poster = get_movie_poster(movie_html_soup)\n",
    "                # Download the poster (optional)\n",
    "                if dwld_poster:\n",
    "                    download_movie_poster(tp_poster, tp_title)\n",
    "                \n",
    "                # Append the data\n",
    "                df_tmp = pd.DataFrame({'id': [tp_id],\n",
    "                                       'title': [tp_title],\n",
    "                                       'release_date': [tp_release_dt],\n",
    "                                       'duration': [tp_duration],\n",
    "                                       'genres': [tp_genre],\n",
    "                                       'directors': [tp_director],\n",
    "                                       'actors': [tp_actor],\n",
    "                                       'nationality': [tp_nation],\n",
    "                                       'press_rating': [tp_press_rating],\n",
    "                                       'nb_press_rating': [tp_nb_press_rating],\n",
    "                                       'spect_rating': [tp_spec_rating],\n",
    "                                       'nb_spect_rating': [tp_nb_spec_rating],\n",
    "                                       'summary': [tp_summary],\n",
    "                                       'poster_link': [tp_poster]})\n",
    "                \n",
    "                df = pd.concat([df, df_tmp], ignore_index=True)                \n",
    "        except:\n",
    "            errors.append(url)\n",
    "            warn(f'Request #{n_request} fail; Total errors : {len(errors)}')\n",
    "            # traceback.print_exc()\n",
    "            \n",
    "    # monitoring \n",
    "    movie_path = '../Movies/Data/'\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(movie_path), exist_ok=True) \n",
    "    elapsed_time = time() - start_time\n",
    "    print(f'--> Done; {n_request} requests in {timedelta(seconds=elapsed_time)} with {len(errors)} errors\\n--> {len(df)} movies successfully scraped')\n",
    "    # Saving files\n",
    "    df.to_csv(f\"{movie_path}allocine_movies.csv\", index=False)\n",
    "    # list to dataframe\n",
    "    errors_df = pd.DataFrame(errors, columns=['url'])\n",
    "    errors_df.to_csv(f\"{movie_path}allocine_errors.csv\",index=False,header=False)\n",
    "    print(f\"Files saved as '{movie_path}allocine_movies.csv' and '{movie_path}allocine_errors.csv'\")\n",
    "    # return dataframe and errors\n",
    "    return df, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## **Launching the script**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting movies urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cabd73e0854eaf86780166be472ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching movies urls:   0%|          | 0/5 [00:00<?, ?pages/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 URLs collected.\n",
      "28/30 URLs collected.\n",
      "41/45 URLs collected.\n",
      "55/60 URLs collected.\n",
      "69/75 URLs collected.\n",
      "--> Done; 5 Page Requests and 69/75 Movie Requests in 0:01:37.143771\n",
      "--> 6 movies not yet released.\n"
     ]
    }
   ],
   "source": [
    "# Scrape the page from start_page to end_page (included) or with nb_pages\n",
    "start_page = 1\n",
    "end_page = None\n",
    "nb_pages = 5\n",
    "movie_url, nr_movies = getMoviesUrl(start_page, end_page=end_page, nb_pages=nb_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the list of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the list of urls \n",
    "m_url = pd.read_csv(\"../Movies/Data/movie_url.csv\",names=['url'])\n",
    "m_url = m_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e15b994aefd4c1fb3c15f81dcb42649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping movies data:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 69 requests in 0:04:17.865416 with 0 errors\n",
      "--> 69 movies successfully scraped\n",
      "Files saved as '../Movies/Data/allocine_movies.csv' and '../Movies/Data/allocine_errors.csv'\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data \n",
    "movies_df, movies_errors = ScrapeURL(m_url)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48c0ee024c6cc2421e377eb4076cadf4ce6eb7a4584917e3645e162e32894617"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
