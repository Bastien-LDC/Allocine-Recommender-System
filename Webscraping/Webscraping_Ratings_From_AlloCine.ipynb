{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping User Ratings From AlloCiné.fr\n",
    "\n",
    "This script builds a DataFrame by web scraping the data from AlloCiné — this time, its purpose is to retrieve the user and press ratings of movies and series from the website of AlloCiné. We will proceed as follows: \n",
    "- We use the series and movies url lists generated in the other scripts to get the comments section urls of the press and the spectators (users) with `getCommentsUrl()`.\n",
    "- We use the press comments urls for movies and series to get the press ratings with `getPressRatings()`.\n",
    "- We use the user comments urls for movies and series to get the user ratings with `getUserRatings()`.\n",
    "- From there, we scrape the user ID and their rating for each movie or series with `ScrapeURL()`.\n",
    "- The user can be either a person or a press newspaper. Separated files are generated for each type of user.\n",
    "\n",
    "*Note : We use the popular BeautifulSoup package*\n",
    "\n",
    "## Functions :\n",
    "\n",
    "### `getCommentsUrl(movies_df, series_df, spect, press)`\n",
    "\n",
    "This function will call two sub-functions: `getMoviesCommentsUrl(movies_df, spect, press)` and `getSeriesCommentsUrl(series_df, spect, press)`. Both respectively iterate over the list of movies and series url generated by `getMoviesUrl()` in the previous scripts and get the comments section url. We can chose to get the comments section from the user or the press for each video type.\n",
    "We then store the lists of urls in a csv file entitled `user_comments_url.csv` (resp. `press_comments_url.csv`), in both movies and series directory (`../Movies/Comments/` and `../Series/Comments/`).\n",
    "\n",
    "### `ScrapeURL(urls)` :\n",
    "\n",
    "Iterate over the list of movies or series comments section url generated by `getCommentsUrl()` and scrape the data for each movie or series ratings. In the process, we extract :\n",
    "\n",
    "- `user_id` : Allocine user id (person or press)\n",
    "- `id` : Allocine movie or series id\n",
    "- `user_rating`: AlloCiné users ratings (from 0.5 to 5 stars) \n",
    "\n",
    "\n",
    "The function `ScrapeURL()` returns two objects : the user_rating and press-rating as two dataframes. In addition, the two objects are saved as `user_ratings_movies.csv` and `press_ratings_movies.csv` (respectively `user_ratings_series.csv` and `press_ratings_series.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------\n",
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\AppData\\Local\\Temp\\ipykernel_29672\\1268171875.py:15: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from time import sleep\n",
    "import re\n",
    "from datetime import timedelta\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import dateparser\n",
    "\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SUPPRIMER APRES TESTS\n",
    "response = urlopen(\"https://www.allocine.fr/film/fichefilm-211012/critiques/spectateurs/\")\n",
    "html_text = response.read().decode(\"utf-8\")\n",
    "spect_html_soup = BeautifulSoup(html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(spect_html_soup.find_all('span', {'class': 'button-md'})[-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert French months to English months: `convert_month(month)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert french months to english months\n",
    "def convert_month(month):\n",
    "    if month == \"janvier\":\n",
    "        return \"January\"\n",
    "    elif month == \"février\":\n",
    "        return \"February\"\n",
    "    elif month == \"mars\":\n",
    "        return \"March\"\n",
    "    elif month == \"avril\":\n",
    "        return \"April\"\n",
    "    elif month == \"mai\":\n",
    "        return \"May\"\n",
    "    elif month == \"juin\":\n",
    "        return \"June\"\n",
    "    elif month == \"juillet\":\n",
    "        return \"July\"\n",
    "    elif month == \"août\":\n",
    "        return \"August\"\n",
    "    elif month == \"septembre\":\n",
    "        return \"September\"\n",
    "    elif month == \"octobre\":\n",
    "        return \"October\"\n",
    "    elif month == \"novembre\":\n",
    "        return \"November\"\n",
    "    elif month == \"décembre\":\n",
    "        return \"December\"\n",
    "    else:\n",
    "        return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `getCommentsUrl(movies_df, series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `getMoviesCommentsUrl(movies_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoviesCommentsUrl(movies_df: pd.DataFrame, spect: bool=False, press: bool=False):\n",
    "    '''\n",
    "    Get the comments section url for each movie\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param movies_df: DataFrame of movies\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in csv files\n",
    "    '''    \n",
    "    # Get the list of movies_id from the movies_df\n",
    "    movies_id_list = movies_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = 1\n",
    "        \n",
    "    for v_id in movies_id_list:\n",
    "        if spect:\n",
    "            # Url for the spectators/user section sorted by the descencing number of reviews per user\n",
    "            url_spect = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/spectateurs/membres-critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Movies/Comments/'\n",
    "    os.makedirs(os.path.dirname(comments_path), exist_ok=True) #create folders if not exists\n",
    "    print(f'--> Done; {p_requests-1} Movies Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    if spect:\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `getSeriesCommentsUrl(series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeriesCommentsUrl(series_df: pd.DataFrame, spect: bool=False, press: bool=False):\n",
    "    '''\n",
    "    Get the comments section url for each series\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param series_df: DataFrame of series\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in  csv files\n",
    "    '''\n",
    "    # Get the list of series_id from the series_df\n",
    "    series_id_list = series_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = 1\n",
    "        \n",
    "    for v_id in series_id_list:\n",
    "        if spect:\n",
    "            # Url for the spectators/user section sorted by the descencing number of reviews per user\n",
    "            url_spect = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/membres-critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Series/Comments/'\n",
    "    os.makedirs(os.path.dirname(comments_path), exist_ok=True) #create folders if not exists\n",
    "    print(f'--> Done; {p_requests-1} Series Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    if spect:\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function: `getCommentsUrl(movies_df, series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommentsUrl(movies_df=None, series_df=None, spect=False, press=False):\n",
    "    '''\n",
    "    Get the comments section url for each movie\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param movies_df: DataFrame of movies\n",
    "    :param series_df: DataFrame of series\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in a csv file\n",
    "    '''\n",
    "    try:\n",
    "        if movies_df is not None:\n",
    "            getMoviesCommentsUrl(movies_df, spect, press)\n",
    "        if series_df is not None:\n",
    "            getSeriesCommentsUrl(series_df, spect, press)\n",
    "    except:\n",
    "        print('Error in getCommentsUrl function!')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get press-ratings dataframe: `getPressRatings(urls)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to rating: `convertTextToRating(text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_rating(text):\n",
    "    '''\n",
    "    Convert text to rating\n",
    "    :param text: Evaluation of the movie as a string \n",
    "    :return: corresponding float rating value\n",
    "    '''\n",
    "    if text==\"Nul\":\n",
    "        return 0.5\n",
    "    elif text==\"Très mauvais\":\n",
    "        return 1\n",
    "    elif text==\"Mauvais\":\n",
    "        return 1.5\n",
    "    elif text==\"Pas terrible\":\n",
    "        return 2\n",
    "    elif text==\"Moyen\":\n",
    "        return 2.5  \n",
    "    elif text==\"Pas mal\":\n",
    "        return 3\n",
    "    elif text==\"Bien\":\n",
    "        return 3.5\n",
    "    elif text==\"Très bien\":\n",
    "        return 4\n",
    "    elif text==\"Excellent\":\n",
    "        return 4.5\n",
    "    elif text==\"Chef d'oeuvre\":\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function: `getPressRatings(urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPressRatings(press_soup):\n",
    "    '''\n",
    "    Get press_ratings from the comments section url for each movie or series.\n",
    "    :param press_soup: BeautifulSoup object of the press comments section url\n",
    "    :return: a dictionary of ratings (key: user_id, value: press rating) if the ratings are available, else None.\n",
    "    '''\n",
    "    press_ratings = {}\n",
    "    div_ratings = press_soup.find_all('li', {'class': 'item'})\n",
    "    if div_ratings:\n",
    "        press_ratings = {id.text.strip():convert_text_to_rating(rating.find('span')['title']) for id, rating in zip(div_ratings, div_ratings)}\n",
    "        return press_ratings\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get user-ratings dataframe: `getUserRatings(urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserRatings(user_rating_url, last_page: int=1, nb_users: int=5):\n",
    "    '''\n",
    "    Get user_ratings from the comments section url for each movie or series.\n",
    "    We will keep only the nb_users first users who have posted the most reviews.\n",
    "    :param user_rating_url : url of the user ratings section.\n",
    "    :param last_page: last page of the user ratings section (default 1).\n",
    "    :param nb_users: Number of users to keep (default 5).\n",
    "    :return: a dictionary of ratings {key=(user_id, user_name): value=(user rating, date)} if the ratings are available, else None.\n",
    "    '''\n",
    "    user_ratings = {}\n",
    "    for n_page in range(1,last_page+1):\n",
    "        # If the number of users is reached, stop the loop.\n",
    "        # Else, get the next page of the user ratings section until the last page is reached.\n",
    "        if len(user_ratings) == nb_users:       \n",
    "            break\n",
    "\n",
    "        user_rating_url_page = user_rating_url + f'?page={n_page}'        \n",
    "        try :\n",
    "            response = urlopen(user_rating_url_page)\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            html_text = response.read().decode(\"utf-8\")\n",
    "            user_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "            div_ratings = user_soup.find_all('div', {'class': 'review-card'})\n",
    "            if div_ratings:\n",
    "                for item in div_ratings:\n",
    "                    if len(user_ratings) == nb_users:       \n",
    "                        break      \n",
    "                    user_id = item.find('span', {'class': 'item-profil'})['data-targetuserid']\n",
    "                    user_name = item.span['title']\n",
    "                    user_rating = float(re.sub(\",\", \".\", item.find(\"span\", {\"class\": \"stareval-note\"}).text))\n",
    "                    user_date = item.find('span', {'class': 'review-card-meta-date'}).text.strip().replace('Publiée le ', '')\n",
    "                    month = user_date.split(' ')[1]\n",
    "                    user_date = dateparser.parse(user_date.replace(month, convert_month(month)), date_formats=[\"%d %B %Y\"]).date()\n",
    "                    user_ratings[(user_id, user_name)] = (user_rating, user_date)\n",
    "        except:\n",
    "            print(f'Error in getUserRatings function!')\n",
    "            traceback.print_exc()\n",
    "    return user_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `ScrapeURL(series_urls, movies_urls, press, spect)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `ScrapePressURL(series_urls, movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapePressURL(series_url: list=None, movies_url: list=None):   \n",
    "    '''\n",
    "    Scrape the press_ratings from the series and/or movies press comments section urls (if available).\n",
    "    :param series_url: list of series press comments section urls.\n",
    "    :param movies_url: list of movies press comments section urls.\n",
    "    :return: dataframes of series and movies press ratings and the errors.\n",
    "    '''\n",
    "    # init the dataframes    \n",
    "    if series_url is None and movies_url is None:\n",
    "        print('No url to scrape!')\n",
    "        return None, None, None, None\n",
    "    if series_url is not None:\n",
    "        c = [\"press_id\",\n",
    "        \"series_id\",\n",
    "        \"press_rating\",]\n",
    "        df_series = pd.DataFrame(columns=c)\n",
    "    if movies_url is not None:\n",
    "        c = [\"press_id\",\n",
    "        \"movie_id\",\n",
    "        \"press_rating\",]\n",
    "        df_movies = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    ns_request = 0\n",
    "    nm_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors_series = []\n",
    "    errors_movies = []\n",
    "    \n",
    "    if series_url is not None:\n",
    "        # request loop\n",
    "        for url in series_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                ns_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Series Press Request #{ns_request}; Frequency: {ns_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Series Press Request #{}; Status code: {}'.format(ns_request, response.status_code))\n",
    "                    errors_series.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the series_id\n",
    "                series_id = url.split('/')[-4].split('-')[-1]            \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_series_tmp = pd.DataFrame({'press_id': [id],\n",
    "                                        'series_id': [series_id],\n",
    "                                        'press_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_series = pd.concat([df_series, df_series_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_series.append(url)\n",
    "                warn(f'Series Press Request #{ns_request} fail; Press rating does not exist! Total errors : {len(errors_series)}')\n",
    "                traceback.print_exc()\n",
    "    if movies_url is not None:\n",
    "        # request loop\n",
    "        for url in movies_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                nm_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Movie Press Request #{nm_request}; Frequency: {nm_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Movie Press Request #{}; Status code: {}'.format(nm_request, response.status_code))\n",
    "                    errors_movies.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the movie_id\n",
    "                movie_id = url.split('/')[-4].split('-')[-1]            \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_movies_tmp = pd.DataFrame({'press_id': [id],\n",
    "                                        'movie_id': [movie_id],\n",
    "                                        'press_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_movies = pd.concat([df_movies, df_movies_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_movies.append(url)\n",
    "                warn(f'Movie Press Request #{nm_request} fail; Press rating does not exist! Total errors : {len(errors_movies)}')\n",
    "                traceback.print_exc()\n",
    "            \n",
    "    # monitoring \n",
    "    if series_url is not None:\n",
    "        series_path = '../Series/Ratings/'\n",
    "        os.makedirs(os.path.dirname(series_path), exist_ok=True) #create folders if not exists\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {ns_request} Series Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_series)} errors (series with no press ratings)')\n",
    "        df_series.to_csv(f\"{series_path}press_ratings_series.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_series = pd.DataFrame(errors_series, columns=['url'])\n",
    "        errors_df_series.to_csv(f\"{series_path}press_ratings_errors.csv\")\n",
    "    if movies_url is not None:\n",
    "        movies_path = '../Movies/Ratings/'\n",
    "        os.makedirs(os.path.dirname(movies_path), exist_ok=True) #create folders if not exists\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {nm_request} Movies Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_movies)} errors (movies with no press ratings)')\n",
    "        df_movies.to_csv(f\"{movies_path}press_ratings_movies.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_movies = pd.DataFrame(errors_movies, columns=['url'])\n",
    "        errors_df_movies.to_csv(f\"{movies_path}press_ratings_errors.csv\")\n",
    "    \n",
    "    # return dataframe and errors\n",
    "    return df_series, df_movies, errors_series, errors_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `ScrapeUserURL(series_urls, movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeUserURL(series_url: list=None, movies_url: list=None):\n",
    "    '''\n",
    "    Scrape the user_ratings from the series and/or movies press comments section urls (if available).\n",
    "    :param series_url: list of series user comments urls\n",
    "    :param movies_url: list of movies user comments urls\n",
    "    :return: dataframes of series and movies user ratings and the errors.  \n",
    "    '''\n",
    "    # init the dataframes    \n",
    "    if series_url is None and movies_url is None:\n",
    "        print('No url to scrape!')\n",
    "        return None, None, None, None\n",
    "    if series_url is not None:\n",
    "        c = [\"user_id\",\n",
    "        \"user_name\",\n",
    "        \"series_id\",\n",
    "        \"user_rating\",\n",
    "        \"date\",\n",
    "    ]\n",
    "        df_series = pd.DataFrame(columns=c)\n",
    "    if movies_url is not None:\n",
    "        c = [\"user_id\",\n",
    "        \"user_name\",\n",
    "        \"movie_id\",\n",
    "        \"user_rating\",\n",
    "        \"date\",\n",
    "    ]\n",
    "        df_movies = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    ns_request = 0\n",
    "    nm_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors_series = []\n",
    "    errors_movies = []\n",
    "    \n",
    "    if series_url is not None:\n",
    "        # request loop\n",
    "        for url in series_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                ns_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Series User Request #{ns_request}; Frequency: {ns_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Series User Request #{}; Status code: {}'.format(ns_request, response.status_code))\n",
    "                    errors_series.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                user_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the series_id\n",
    "                series_id = url.split('/')[-4].split('-')[-1] \n",
    "                # Get the url last page\n",
    "                last_page = int(user_html_soup.find_all('span', {'class': 'button-md'})[-1].text)\n",
    "                # Get the user_ratings\n",
    "                user_ratings = getUserRatings(url,last_page,nb_users=10)\n",
    "                \n",
    "                for id, rating in user_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_series_tmp = pd.DataFrame({'user_id': [id[0]],\n",
    "                                        'user_name': [id[1]],\n",
    "                                        'series_id': [series_id],\n",
    "                                        'user_rating': [rating[0]],\n",
    "                                        'date': [rating[1]]\n",
    "                                        })                                    \n",
    "                    df_series = pd.concat([df_series, df_series_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_series.append(url)\n",
    "                warn(f'Series User Request #{ns_request} fail; User rating does not exist! Total errors : {len(errors_series)}')\n",
    "                traceback.print_exc()\n",
    "\n",
    "    if movies_url is not None:\n",
    "        # request loop\n",
    "        for url in movies_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                nm_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Movie User Request #{nm_request}; Frequency: {nm_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Movie User Request #{}; Status code: {}'.format(nm_request, response.status_code))\n",
    "                    errors_movies.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                user_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the movie_id\n",
    "                movie_id = url.split('/')[-5].split('-')[-1]            \n",
    "                # Get the url last page\n",
    "                last_page = int(user_html_soup.find_all('span', {'class': 'button-md'})[-1].text)\n",
    "                # Get the user_ratings\n",
    "                user_ratings = getUserRatings(url,last_page,nb_users=10)\n",
    "                \n",
    "                for id, rating in user_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_movies_tmp = pd.DataFrame({'user_id': [id[0]],\n",
    "                                        'user_name': [id[1]],\n",
    "                                        'movie_id': [movie_id],\n",
    "                                        'user_rating': [rating[0]],\n",
    "                                        'date': [rating[1]]\n",
    "                                        })                                    \n",
    "                    df_movies = pd.concat([df_movies, df_movies_tmp], ignore_index=True)                             \n",
    "            except:\n",
    "                errors_movies.append(url)\n",
    "                warn(f'Movie User Request #{nm_request} fail; User rating does not exist! Total errors : {len(errors_movies)}')\n",
    "                traceback.print_exc()\n",
    "            \n",
    "    # monitoring \n",
    "    if series_url is not None:\n",
    "        series_path = '../Series/Ratings/'\n",
    "        os.makedirs(os.path.dirname(series_path), exist_ok=True) #create folders if not exists\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {ns_request} Series User Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_series)} errors (series with no user ratings)')\n",
    "        df_series.to_csv(f\"{series_path}user_ratings_series.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_series = pd.DataFrame(errors_series, columns=['url'])\n",
    "        errors_df_series.to_csv(f\"{series_path}user_ratings_errors.csv\")\n",
    "    if movies_url is not None:\n",
    "        movies_path = '../Movies/Ratings/'\n",
    "        os.makedirs(os.path.dirname(movies_path), exist_ok=True) #create folders if not exists\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {nm_request} Movies User Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_movies)} errors (movies with no user ratings)')\n",
    "        df_movies.to_csv(f\"{movies_path}user_ratings_movies.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_movies = pd.DataFrame(errors_movies, columns=['url'])\n",
    "        errors_df_movies.to_csv(f\"{movies_path}user_ratings_errors.csv\")\n",
    "    \n",
    "    # return dataframe and errors\n",
    "    return df_series, df_movies, errors_series, errors_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function: `ScrapeURL(press_series_urls, press_movies_urls, user_series_urls, user_movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeURL(press_series_urls: list=None, press_movies_urls: list=None, user_series_urls: list=None, user_movies_urls: list=None):\n",
    "    '''\n",
    "    Scrape the user and press ratings from the movie and series pages.\n",
    "    :param press_series_urls: list of press series urls\n",
    "    :param press_movies_urls: list of press movies urls\n",
    "    :param user_series_urls: list of user series urls\n",
    "    :param user_movies_urls: list of user movies urls\n",
    "    :return: list of dataframes of user and press ratings and errors\n",
    "    '''\n",
    "    ratings,errors = [],[]  \n",
    "    print('Scraping Press Ratings...')\n",
    "    press_series_ratings, press_movies_ratings, press_errors_series, press_errors_movies = ScrapePressURL(press_series_urls, press_movies_urls)\n",
    "    ratings.append(press_series_ratings)\n",
    "    ratings.append(press_movies_ratings)\n",
    "    errors.append(press_errors_series)\n",
    "    errors.append(press_errors_movies)\n",
    "\n",
    "    print('Scraping Spectator Ratings...')\n",
    "    user_series_ratings, user_movies_ratings, user_errors_series, user_errors_movies = ScrapeUserURL(user_series_urls, user_movies_urls)\n",
    "    ratings.append(user_series_ratings)\n",
    "    ratings.append(user_movies_ratings)\n",
    "    errors.append(user_errors_series)\n",
    "    errors.append(user_errors_movies)\n",
    "    return ratings, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the movies and series dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movies and series dataframes\n",
    "def loadDataFrames():\n",
    "    '''\n",
    "    Load the movies and series dataframes\n",
    "    :return: movies and series dataframes\n",
    "    '''\n",
    "    movies_df = pd.read_csv('../Movies/Data/allocine_movies.csv', nrows=15)\n",
    "    series_df = pd.read_csv('../Series/Data/allocine_series.csv', nrows=15)\n",
    "    return movies_df, series_df\n",
    "movies_df, series_df = loadDataFrames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'En corps': 'https://fr.web.img4.acsta.net/c_310_420/pictures/22/02/09/15/22/3063490.jpg',\n",
       " 'The Batman': 'https://fr.web.img3.acsta.net/c_310_420/pictures/22/02/16/17/42/3125788.jpg',\n",
       " 'Goliath': 'https://fr.web.img3.acsta.net/c_310_420/pictures/22/01/24/12/13/4677076.jpg',\n",
       " 'Notre-Dame brûle': 'https://fr.web.img6.acsta.net/c_310_420/pictures/22/02/11/12/58/0195580.jpg',\n",
       " 'Morbius': 'https://fr.web.img5.acsta.net/c_310_420/pictures/22/03/28/09/03/5612671.jpg',\n",
       " \"Qu'est-ce qu'on a tous fait au Bon Dieu ?\": 'https://fr.web.img5.acsta.net/c_310_420/pictures/21/10/21/10/17/5465951.jpg',\n",
       " 'Permis de construire': 'https://fr.web.img6.acsta.net/c_310_420/pictures/21/10/18/09/57/1868139.jpg',\n",
       " 'Le Temps des secrets': 'https://fr.web.img3.acsta.net/c_310_420/pictures/21/12/23/08/44/0309697.jpg',\n",
       " 'Sonic 2 le film': 'https://fr.web.img3.acsta.net/c_310_420/pictures/22/03/14/15/39/4137538.jpg',\n",
       " 'Ambulance': 'https://fr.web.img6.acsta.net/c_310_420/pictures/22/02/21/18/59/0222383.jpg',\n",
       " 'À plein temps': 'https://fr.web.img2.acsta.net/c_310_420/pictures/22/02/16/14/57/2354541.jpg',\n",
       " 'La Brigade': 'https://fr.web.img5.acsta.net/c_310_420/pictures/22/02/09/15/16/1331267.jpg',\n",
       " 'Les Animaux Fantastiques : les Secrets de Dumbledore': 'https://fr.web.img6.acsta.net/c_310_420/pictures/22/03/16/15/20/0170262.jpg',\n",
       " 'En même temps': 'https://fr.web.img6.acsta.net/c_310_420/pictures/22/03/02/10/16/2322243.jpg',\n",
       " 'Jujutsu Kaisen 0': 'https://fr.web.img4.acsta.net/c_310_420/pictures/22/02/16/12/44/0721822.png'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poster_links = {m_name:p_link for m_name, p_link in zip(movies_df[\"title\"], movies_df[\"poster_link\"])}\n",
    "poster_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the comments section urls from spectators and press for movies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 15 Movies Comments Page Requests in 0:00:00.001034\n",
      "--> Done; 15 Series Comments Page Requests in 0:00:00.001015\n"
     ]
    }
   ],
   "source": [
    "getCommentsUrl(movies_df=movies_df, series_df=series_df, spect=True, press=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the comments section urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the press_comments_urls for series\n",
    "press_series_url = pd.read_csv(\"../Series/Comments/press_comments_urls.csv\",names=['url'])\n",
    "press_series_url = press_series_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the user_comments_urls for series\n",
    "user_series_url = pd.read_csv(\"../Series/Comments/user_comments_urls.csv\",names=['url'])\n",
    "user_series_url = user_series_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the press_comments_urls for movies\n",
    "press_movies_url = pd.read_csv(\"../Movies/Comments/press_comments_urls.csv\",names=['url'])\n",
    "press_movies_url = press_movies_url['url'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the users_comments_urls for movies\n",
    "user_movies_url = pd.read_csv(\"../Movies/Comments/user_comments_urls.csv\",names=['url'])\n",
    "user_movies_url = user_movies_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done; 15 Series User Ratings requests in 0:04:25.685541 with 1 errors (series with no user ratings)\n",
      "Done; 15 Movies User Ratings requests in 0:04:25.691344 with 0 errors (movies with no user ratings)\n"
     ]
    }
   ],
   "source": [
    "# Scraping the user and/or press ratings from series and/or movies\n",
    "ratings, errors = ScrapeURL(press_series_urls=None, press_movies_urls=None, user_series_urls=user_series_url, user_movies_urls=user_movies_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "press_series_ratings = ratings[0]\n",
    "press_movies_ratings = ratings[1]\n",
    "user_series_ratings = ratings[2]\n",
    "user_movies_ratings = ratings[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_series_ratings.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variety                    75\n",
       "Télérama                   74\n",
       "The Hollywood Reporter     57\n",
       "Le Parisien                57\n",
       "Le Monde                   51\n",
       "                           ..\n",
       "Season One                  1\n",
       "Nice-Matin                  1\n",
       "Philadelphia Daily News     1\n",
       "Red Eye                     1\n",
       "The Salt Lake Tribune       1\n",
       "Name: user_id, Length: 120, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort df_press_ratings by user_id\n",
    "# press_series_ratings = press_series_ratings.sort_values(by=['user_id'],ignore_index=True)\n",
    "# press_series_ratings.user_id.value_counts()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff1774c64d83fce3825259fa3771bbe70271854497325f5fa1e2c1b92279703"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
