{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎦**Web Scraping User Ratings From AlloCiné.fr**🏆⭐⭐⭐⭐⭐\n",
    "\n",
    "After building our movies and series dataframes in the previous notebooks (see [Webscraping Movies](https://github.com/Bastien-LDC/Allocine-Recommender-System/blob/master/Webscraping/Webscraping_Movies_From_AlloCine.ipynb) and [Webscraping Series](https://github.com/Bastien-LDC/Allocine-Recommender-System/blob/master/Webscraping/Webscraping_Series_From_AlloCine.ipynb)), this time we will retrieve the **user and press ratings** of movies and series from the website of AlloCiné. We will proceed as follows: \n",
    "- We reconstruct the comments section urls of the press and the spectators for each movies and/or series with the function `getCommentsUrl(...)`.\n",
    "- From there, we scrape the rating data from the press and/or user page for each movie and/or series with `ScrapeURL(...)`. \n",
    "- The user can be either a person or a press newspaper. Separated files are generated for each type of user, whether for movies or series.\n",
    "\n",
    "*📝Note 1: We use the popular BeautifulSoup package*\n",
    "\n",
    "## **Functions :**\n",
    "\n",
    "### `getCommentsUrl(movies_df, series_df, spect, press)` :\n",
    "\n",
    "This function will call two sub-functions: `getMoviesCommentsUrl(movies_df, spect, press)` and `getSeriesCommentsUrl(series_df, spect, press)`. Both functions respectively retrieve the list of movies and series ID in the movies and series dataframes generated by `getMoviesUrl(start_page, end_page, nb_pages)` and `getSeriesUrl(start_page, end_page, nb_pages)` in the previous scripts. Then, we use the IDs to reconstruct the comments section urls of the press and the spectators (users). We can chose to get the comments section from the user or the press for each video type.\n",
    "We then store the lists of urls in a csv file entitled `user_comments_url.csv` (resp. `press_comments_url.csv`), in both movies and series directory (`../Movies/Comments/` and `../Series/Comments/`).\n",
    "\n",
    "### `ScrapeURL(press_series_urls, press_movies_urls, user_series_urls, user_movies_urls, nb_users)` :\n",
    "\n",
    "This function iterates over the list of movies or series comments section urls generated by `getCommentsUrl(movies_df, series_df, spect, press)` and scrape the press and user rating data for each video type, by calling respectively `ScrapePressURL(series_urls, movies_urls)` and `ScrapeUserURL(series_urls, movies_urls)`. You can choose whether to scrape the press or the user ratings, whether for movies or series, or both, depending on the parameters value as input of the `ScrapeURL(...)` function.\n",
    "We use the press comments urls for movies and series to get the press ratings with `getPressRatings(press_soup)`.\n",
    "We use the user comments urls for movies and series to get the user ratings with `getUserRatings(user_rating_url, last_page, nb_users)`.\n",
    "\n",
    "In the process, we extract, if available :\n",
    "- `user_id`: AlloCiné user id (unavailable for the press)\n",
    "- `(user/press)_name`: AlloCiné user/press name\n",
    "- `(movie/series)_id`: AlloCiné movie/series id\n",
    "- `(user/press)_rating`: AlloCiné user/press rating (from 0.5 to 5 stars ⭐⭐⭐⭐⭐)\n",
    "- `date`: date of the rating (unavailable for the press)\n",
    "\n",
    "The function `ScrapeURL(...)` returns two objects : one list containing the 4 generated ratings dataframes (user_rating_movies, press_rating_movies, user_rating_series, press_rating_series), and another list of the respective errors (as dataframes) for those ratings. In addition, the eight dataframes are saved as `user_ratings_movies.csv` and `press_ratings_movies.csv` (respectively `user_ratings_series.csv` and `press_ratings_series.csv`), and as `press_ratings_errors` and `user_ratings_errors` (in both `../Movies/Ratings/` and `../Series/Ratings/`).\n",
    "\n",
    "(⚠️ **Warning** ⚠️: the process can take a while, depending on the number of movies or series and the number of user pages you choose to scrape. It is recommended to use a dedicated computer for this process, as it can take a long time to complete.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## **Import libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\AppData\\Local\\Temp\\ipykernel_25132\\970916280.py:15: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from time import sleep\n",
    "import re\n",
    "from datetime import timedelta, date\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import dateparser\n",
    "\n",
    "from warnings import warn, filterwarnings\n",
    "from IPython.core.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SUPPRIMER APRES TESTS\n",
    "response = urlopen(\"https://www.allocine.fr/series/ficheserie-18529/critiques/membres-critiques/\")\n",
    "html_text = response.read().decode(\"utf-8\")\n",
    "spect_html_soup = BeautifulSoup(html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## **Function:** Getting the comments section urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get Movie Comments Section: `getMoviesCommentsUrl(movies_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoviesCommentsUrl(movies_df: pd.DataFrame, spect: bool=False, press: bool=False) -> None:\n",
    "    '''\n",
    "    Get the comments section url for each movie\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param movies_df: DataFrame of movies\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in csv files\n",
    "    '''    \n",
    "    # Get the list of movies_id from the movies_df\n",
    "    movies_id_list = movies_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = 1\n",
    "        \n",
    "    for v_id in movies_id_list:\n",
    "        if spect:\n",
    "            # Url for the spectators/user section sorted by the descending number of reviews per user\n",
    "            url_spect = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/spectateurs/membres-critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Movies/Comments/'\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(comments_path), exist_ok=True)\n",
    "    print(f'--> Done; {p_requests-1} Movies Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    if spect:\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get Series Comments Section: `getSeriesCommentsUrl(series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeriesCommentsUrl(series_df: pd.DataFrame, spect: bool=False, press: bool=False) -> None:\n",
    "    '''\n",
    "    Get the comments section url for each series\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param series_df: DataFrame of series\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in csv files\n",
    "    '''\n",
    "    # Get the list of series_id from the series_df\n",
    "    series_id_list = series_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = 1\n",
    "        \n",
    "    for v_id in series_id_list:\n",
    "        if spect:\n",
    "            # Url for the spectators/user section sorted by the descending number of reviews per user\n",
    "            url_spect = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/membres-critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Series/Comments/'\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(comments_path), exist_ok=True)\n",
    "    print(f'--> Done; {p_requests-1} Series Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    if spect:\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Main function: `getCommentsUrl(movies_df, series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommentsUrl(movies_df: pd.DataFrame=None, series_df: pd.DataFrame=None, spect: bool=False, press: bool=False) -> None:\n",
    "    '''\n",
    "    Get the comments section url for each movie and series\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param movies_df: DataFrame of movies\n",
    "    :param series_df: DataFrame of series\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in a csv file\n",
    "    '''\n",
    "    try:\n",
    "        if movies_df is not None:\n",
    "            getMoviesCommentsUrl(movies_df, spect, press)\n",
    "        if series_df is not None:\n",
    "            getSeriesCommentsUrl(series_df, spect, press)\n",
    "    except:\n",
    "        print('Error in getCommentsUrl function!')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Functions:** Getting ratings infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Convert French months to English months: `convert_month(month)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_month(month: str) -> str:\n",
    "    '''\n",
    "    Convert French months into English months for the dateparser to work.\n",
    "    :param month: French month.\n",
    "    :return: English month.'''\n",
    "    if month == \"janvier\":\n",
    "        return \"January\"\n",
    "    elif month == \"février\":\n",
    "        return \"February\"\n",
    "    elif month == \"mars\":\n",
    "        return \"March\"\n",
    "    elif month == \"avril\":\n",
    "        return \"April\"\n",
    "    elif month == \"mai\":\n",
    "        return \"May\"\n",
    "    elif month == \"juin\":\n",
    "        return \"June\"\n",
    "    elif month == \"juillet\":\n",
    "        return \"July\"\n",
    "    elif month == \"août\":\n",
    "        return \"August\"\n",
    "    elif month == \"septembre\":\n",
    "        return \"September\"\n",
    "    elif month == \"octobre\":\n",
    "        return \"October\"\n",
    "    elif month == \"novembre\":\n",
    "        return \"November\"\n",
    "    elif month == \"décembre\":\n",
    "        return \"December\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Convert text to rating: `convertTextToRating(text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_rating(text: str) -> float:\n",
    "    '''\n",
    "    Convert the press evaluation text into a rating.\n",
    "    :param text: Evaluation of the movie as a string \n",
    "    :return: corresponding float rating value\n",
    "    '''\n",
    "    if text==\"Nul\":\n",
    "        return 0.5\n",
    "    elif text==\"Très mauvais\":\n",
    "        return 1.0\n",
    "    elif text==\"Mauvais\":\n",
    "        return 1.5\n",
    "    elif text==\"Pas terrible\":\n",
    "        return 2.0\n",
    "    elif text==\"Moyen\":\n",
    "        return 2.5  \n",
    "    elif text==\"Pas mal\":\n",
    "        return 3.0\n",
    "    elif text==\"Bien\":\n",
    "        return 3.5\n",
    "    elif text==\"Très bien\":\n",
    "        return 4.0\n",
    "    elif text==\"Excellent\":\n",
    "        return 4.5\n",
    "    elif text==\"Chef-d'oeuvre\":\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get press-ratings: `getPressRatings(press_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPressRatings(press_soup: BeautifulSoup) -> dict[str, float]:\n",
    "    '''\n",
    "    Get press_ratings from the comments section url for each movie or series.\n",
    "    :param press_soup: BeautifulSoup object of the press comments section url\n",
    "    :return: a dictionary of ratings (key: user_id, value: press rating) if the ratings are available, else None.\n",
    "    '''\n",
    "    press_ratings = {}\n",
    "    div_ratings = press_soup.find_all('li', {'class': 'item'})\n",
    "    if div_ratings:\n",
    "        press_ratings = {div.text.strip():convert_text_to_rating(div.find('span')['title']) for div in div_ratings}\n",
    "        return press_ratings\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get user-ratings: `getUserRatings(user_rating_url, last_page, nb_users)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserRatings(user_rating_url: str, last_page: int=1, nb_users: int=5) -> dict[tuple[str,str]: tuple[float,date]]:\n",
    "    '''\n",
    "    Get user_ratings from the comments section url for each movie or series.\n",
    "    We will keep only the nb_users first users who have posted the most reviews.\n",
    "    :param user_rating_url : url of the user ratings section.\n",
    "    :param last_page: last page of the user ratings section (default 1).\n",
    "    :param nb_users: Number of users to keep (default 5).\n",
    "    :return: a dictionary of ratings {key=(user_id, user_name): value=(user rating, date)} if the ratings are available, else None.\n",
    "    '''\n",
    "    user_ratings = {}\n",
    "    for n_page in range(1,last_page+1):\n",
    "        # If the number of users is reached, stop the loop.\n",
    "        # Else, get the next page of the user ratings section until the last page is reached.\n",
    "        if len(user_ratings) == nb_users:       \n",
    "            break\n",
    "\n",
    "        user_rating_url_page = user_rating_url + f'?page={n_page}'        \n",
    "        try :\n",
    "            response = urlopen(user_rating_url_page)\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            html_text = response.read().decode(\"utf-8\")\n",
    "            user_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "            # Collect only the reviews infos from the page in a list\n",
    "            div_ratings = user_soup.find_all('div', {'class': 'review-card'})\n",
    "            if div_ratings:\n",
    "                for item in div_ratings:\n",
    "                    # If the number of users is reached, stop the loop.\n",
    "                    if len(user_ratings) == nb_users:       \n",
    "                        break   \n",
    "                    # If the user is not a visitor...\n",
    "                    if item.find('span', {'class': 'item-profil'}):\n",
    "                        # ...Get the user id, name, rating and date of the rating   \n",
    "                        user_id = item.find('span', {'class': 'item-profil'})['data-targetuserid']\n",
    "                        user_name = item.span['title']\n",
    "                        user_rating = float(re.sub(\",\", \".\", item.find(\"span\", {\"class\": \"stareval-note\"}).text))\n",
    "                        user_date = item.find('span', {'class': 'review-card-meta-date'}).text.strip().replace('Publiée le ', '')\n",
    "                        # Convert the date from French to English\n",
    "                        month = user_date.split(' ')[1]\n",
    "                        user_date = dateparser.parse(user_date.replace(month, convert_month(month)), date_formats=[\"%d %B %Y\"]).date()\n",
    "                        # Append to the dictionary\n",
    "                        user_ratings[(user_id, user_name)] = (user_rating, user_date)\n",
    "        except:\n",
    "            print(f'Error in getUserRatings function!')\n",
    "            traceback.print_exc()\n",
    "    return user_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Function:** Scraping the ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Scraping Press URLs: `ScrapePressURL(series_urls, movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapePressURL(series_url: list=None, movies_url: list=None) -> tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame]:   \n",
    "    '''\n",
    "    Scrape the press_ratings from the series and/or movies press comments section urls (if available).\n",
    "    :param series_url: list of series press comments section urls.\n",
    "    :param movies_url: list of movies press comments section urls.\n",
    "    :return: dataframes of series and movies press ratings and the errors.\n",
    "    '''\n",
    "    # init the dataframes    \n",
    "    df_series, df_movies, errors_series, errors_movies = None, None, None, None\n",
    "    if series_url is None and movies_url is None:\n",
    "        print('No url to scrape!')\n",
    "        return df_series, df_movies, errors_series, errors_movies\n",
    "    if series_url is not None:\n",
    "        c = [\"press_name\",\n",
    "        \"series_id\",\n",
    "        \"press_rating\",]\n",
    "        df_series = pd.DataFrame(columns=c)\n",
    "    if movies_url is not None:\n",
    "        c = [\"press_name\",\n",
    "        \"movie_id\",\n",
    "        \"press_rating\",]\n",
    "        df_movies = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    ns_request = 0\n",
    "    nm_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors_series = []\n",
    "    errors_movies = []\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Scraping the series press ratings\n",
    "    # -----------------------------------\n",
    "    if series_url is not None:\n",
    "        # request loop\n",
    "        for url in series_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                ns_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Series Press Request #{ns_request}; Frequency: {ns_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Series Press Request #{}; Status code: {}'.format(ns_request, response.status_code))\n",
    "                    errors_series.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the series_id\n",
    "                series_id = url.split('/')[-4].split('-')[-1]            \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_series_tmp = pd.DataFrame({'press_name': [id],\n",
    "                                        'series_id': [series_id],\n",
    "                                        'press_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_series = pd.concat([df_series, df_series_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_series.append(url)\n",
    "                warn(f'Series Press Request #{ns_request} fail; Press rating does not exist! Total errors : {len(errors_series)}')\n",
    "                traceback.print_exc()\n",
    "\n",
    "        # Monitoring \n",
    "        series_path = '../Series/Ratings/'\n",
    "        # We create the folder if not exists\n",
    "        os.makedirs(os.path.dirname(series_path), exist_ok=True)\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {ns_request} Series Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_series)} errors (series with no press ratings)')\n",
    "        # Saving files\n",
    "        df_series.to_csv(f\"{series_path}press_ratings_series.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_series = pd.DataFrame(errors_series, columns=['url'])\n",
    "        errors_df_series.to_csv(f\"{series_path}press_ratings_errors.csv\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # Scraping the movies press ratings\n",
    "    # -----------------------------------\n",
    "    if movies_url is not None:\n",
    "        # request loop\n",
    "        for url in movies_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                nm_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Movie Press Request #{nm_request}; Frequency: {nm_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Movie Press Request #{}; Status code: {}'.format(nm_request, response.status_code))\n",
    "                    errors_movies.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the movie_id\n",
    "                movie_id = url.split('/')[-4].split('-')[-1]            \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_movies_tmp = pd.DataFrame({'press_name': [id],\n",
    "                                        'movie_id': [movie_id],\n",
    "                                        'press_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_movies = pd.concat([df_movies, df_movies_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_movies.append(url)\n",
    "                warn(f'Movie Press Request #{nm_request} fail; Press rating does not exist! Total errors : {len(errors_movies)}')\n",
    "                traceback.print_exc()\n",
    "            \n",
    "        # Monitoring\n",
    "        movies_path = '../Movies/Ratings/'\n",
    "        # We create the folder if not exists\n",
    "        os.makedirs(os.path.dirname(movies_path), exist_ok=True)\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {nm_request} Movies Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_movies)} errors (movies with no press ratings)')\n",
    "        # Saving files\n",
    "        df_movies.to_csv(f\"{movies_path}press_ratings_movies.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_movies = pd.DataFrame(errors_movies, columns=['url'])\n",
    "        errors_df_movies.to_csv(f\"{movies_path}press_ratings_errors.csv\")\n",
    "    \n",
    "    # return dataframe and errors\n",
    "    return df_series, df_movies, errors_series, errors_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Scraping User URLs: `ScrapeUserURL(series_urls, movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeUserURL(series_url: list=None, movies_url: list=None, nb_users: int=5) -> tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "    '''\n",
    "    Scrape the user_ratings from the series and/or movies press comments section urls (if available).\n",
    "    :param series_url: list of series user comments urls\n",
    "    :param movies_url: list of movies user comments urls\n",
    "    :param nb_users: number of users to scrape (default: 5)\n",
    "    :return: dataframes of series and movies user ratings and the errors.  \n",
    "    '''\n",
    "    # init the dataframes    \n",
    "    df_series, df_movies, errors_series, errors_movies = None, None, None, None\n",
    "    if series_url is None and movies_url is None:\n",
    "        print('No url to scrape!')\n",
    "        return df_series, df_movies, errors_series, errors_movies\n",
    "    if series_url is not None:\n",
    "        c = [\"user_id\",\n",
    "        \"user_name\",\n",
    "        \"series_id\",\n",
    "        \"user_rating\",\n",
    "        \"date\",\n",
    "    ]\n",
    "        df_series = pd.DataFrame(columns=c)\n",
    "    if movies_url is not None:\n",
    "        c = [\"user_id\",\n",
    "        \"user_name\",\n",
    "        \"movie_id\",\n",
    "        \"user_rating\",\n",
    "        \"date\",\n",
    "    ]\n",
    "        df_movies = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    ns_request = 0\n",
    "    nm_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors_series = []\n",
    "    errors_movies = []\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Scraping the series user ratings\n",
    "    # -----------------------------------\n",
    "    if series_url is not None:\n",
    "        # request loop\n",
    "        for url in series_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                ns_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Series User Request #{ns_request}; Frequency: {ns_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Series User Request #{}; Status code: {}'.format(ns_request, response.status_code))\n",
    "                    errors_series.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                user_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the series_id\n",
    "                series_id = url.split('/')[-4].split('-')[-1] \n",
    "                # Get the url last page if exists. Default 1.\n",
    "                last_page = int(user_html_soup.find_all('span', {'class': 'button-md'})[-1].text) if user_html_soup.find_all('span', {'class': 'button-md'})  else 1\n",
    "                # Get the user_ratings\n",
    "                user_ratings = getUserRatings(url,last_page,nb_users=nb_users)\n",
    "                \n",
    "                for id, rating in user_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_series_tmp = pd.DataFrame({'user_id': [id[0]],\n",
    "                                        'user_name': [id[1]],\n",
    "                                        'series_id': [series_id],\n",
    "                                        'user_rating': [rating[0]],\n",
    "                                        'date': [rating[1]]\n",
    "                                        })                                    \n",
    "                    df_series = pd.concat([df_series, df_series_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_series.append(url)\n",
    "                warn(f'Series User Request #{ns_request} fail; User rating does not exist! Total errors : {len(errors_series)}')\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Monitoring\n",
    "        series_path = '../Series/Ratings/'\n",
    "        # We create the folder if not exists\n",
    "        os.makedirs(os.path.dirname(series_path), exist_ok=True)\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {ns_request} Series User Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_series)} errors (series with no user ratings)')\n",
    "        # Saving files\n",
    "        df_series.to_csv(f\"{series_path}user_ratings_series.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_series = pd.DataFrame(errors_series, columns=['url'])\n",
    "        errors_df_series.to_csv(f\"{series_path}user_ratings_errors.csv\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # Scraping the movies user ratings\n",
    "    # -----------------------------------\n",
    "    if movies_url is not None:\n",
    "        # request loop\n",
    "        for url in movies_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                nm_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Movie User Request #{nm_request}; Frequency: {nm_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Movie User Request #{}; Status code: {}'.format(nm_request, response.status_code))\n",
    "                    errors_movies.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                user_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the movie_id\n",
    "                movie_id = url.split('/')[-5].split('-')[-1]            \n",
    "                # Get the url last page if exists. Default 1.\n",
    "                last_page = int(user_html_soup.find_all('span', {'class': 'button-md'})[-1].text) if user_html_soup.find_all('span', {'class': 'button-md'})  else 1\n",
    "                # Get the user_ratings\n",
    "                user_ratings = getUserRatings(url,last_page,nb_users=nb_users)\n",
    "                \n",
    "                for id, rating in user_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_movies_tmp = pd.DataFrame({'user_id': [id[0]],\n",
    "                                        'user_name': [id[1]],\n",
    "                                        'movie_id': [movie_id],\n",
    "                                        'user_rating': [rating[0]],\n",
    "                                        'date': [rating[1]]\n",
    "                                        })                                    \n",
    "                    df_movies = pd.concat([df_movies, df_movies_tmp], ignore_index=True)                             \n",
    "            except:\n",
    "                errors_movies.append(url)\n",
    "                warn(f'Movie User Request #{nm_request} fail; User rating does not exist! Total errors : {len(errors_movies)}')\n",
    "                traceback.print_exc()\n",
    "            \n",
    "        # Monitoring\n",
    "        movies_path = '../Movies/Ratings/'\n",
    "        # We create the folder if not exists\n",
    "        os.makedirs(os.path.dirname(movies_path), exist_ok=True) \n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {nm_request} Movies User Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_movies)} errors (movies with no user ratings)')\n",
    "        # Saving files\n",
    "        df_movies.to_csv(f\"{movies_path}user_ratings_movies.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_movies = pd.DataFrame(errors_movies, columns=['url'])\n",
    "        errors_df_movies.to_csv(f\"{movies_path}user_ratings_errors.csv\")\n",
    "    \n",
    "    # return dataframe and errors\n",
    "    return df_series, df_movies, errors_series, errors_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Main function: `ScrapeURL(press_series_urls, press_movies_urls, user_series_urls, user_movies_urls, nb_users)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeURL(press_series_urls: list=None, press_movies_urls: list=None, user_series_urls: list=None, user_movies_urls: list=None, nb_users: int=5) -> tuple[list,list]:\n",
    "    '''\n",
    "    Scrape the user and press ratings from the movie and series pages.\n",
    "    :param press_series_urls: list of press series urls\n",
    "    :param press_movies_urls: list of press movies urls\n",
    "    :param user_series_urls: list of user series urls\n",
    "    :param user_movies_urls: list of user movies urls\n",
    "    :return: 2 lists of dataframes of user and press: one for the ratings \n",
    "    and the other one for the errors.\n",
    "    '''\n",
    "    # We ignore dateparse warnings\n",
    "    filterwarnings(\"ignore\",message=\"The localize method is no longer necessary, as this time zone supports the fold attribute\")\n",
    "    # Init the ratings and errors lists\n",
    "    ratings,errors = [],[]  \n",
    "    print('Scraping Press Ratings...')\n",
    "    press_series_ratings, press_movies_ratings, press_errors_series, press_errors_movies = ScrapePressURL(press_series_urls, press_movies_urls)\n",
    "    ratings.append(press_series_ratings)\n",
    "    ratings.append(press_movies_ratings)\n",
    "    errors.append(press_errors_series)\n",
    "    errors.append(press_errors_movies)\n",
    "\n",
    "    print('Scraping Spectator Ratings...')\n",
    "    user_series_ratings, user_movies_ratings, user_errors_series, user_errors_movies = ScrapeUserURL(user_series_urls, user_movies_urls, nb_users=nb_users)\n",
    "    ratings.append(user_series_ratings)\n",
    "    ratings.append(user_movies_ratings)\n",
    "    errors.append(user_errors_series)\n",
    "    errors.append(user_errors_movies)\n",
    "    return ratings, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## **Launching the script**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the movies and series dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movies and series dataframes\n",
    "def loadDataFrames(nrows: int=None) -> tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    '''\n",
    "    Load the movies and series dataframes\n",
    "    :param nrows: number of rows to load\n",
    "    :return: movies and series dataframes\n",
    "    '''\n",
    "    movies_df = pd.read_csv('../Movies/Data/allocine_movies.csv', nrows=nrows)\n",
    "    series_df = pd.read_csv('../Series/Data/allocine_series.csv', nrows=nrows)\n",
    "    return movies_df, series_df\n",
    "movies_df, series_df = loadDataFrames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting the comments section urls from spectators and press for movies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 15 Movies Comments Page Requests in 0:00:00\n",
      "--> Done; 15 Series Comments Page Requests in 0:00:00\n"
     ]
    }
   ],
   "source": [
    "getCommentsUrl(movies_df=movies_df, series_df=series_df, spect=True, press=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the comments section urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the press_comments_urls for series\n",
    "press_series_urls = pd.read_csv(\"../Series/Comments/press_comments_urls.csv\",names=['url'])\n",
    "press_series_urls = press_series_urls['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the user_comments_urls for series\n",
    "user_series_urls = pd.read_csv(\"../Series/Comments/user_comments_urls.csv\",names=['url'])\n",
    "user_series_urls = user_series_urls['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the press_comments_urls for movies\n",
    "press_movies_urls = pd.read_csv(\"../Movies/Comments/press_comments_urls.csv\",names=['url'])\n",
    "press_movies_urls = press_movies_urls['url'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the users_comments_urls for movies\n",
    "user_movies_urls = pd.read_csv(\"../Movies/Comments/user_comments_urls.csv\",names=['url'])\n",
    "user_movies_urls = user_movies_urls['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done; 15 Movies User Ratings requests in 0:02:36.408385 with 0 errors (movies with no user ratings)\n"
     ]
    }
   ],
   "source": [
    "# Scraping the user and/or press ratings from series and/or movies\n",
    "ratings, errors = ScrapeURL(press_series_urls=press_series_urls, \n",
    "                            press_movies_urls=press_movies_urls, \n",
    "                            user_series_urls=user_series_urls, \n",
    "                            user_movies_urls=user_movies_urls,\n",
    "                            )\n",
    "press_series_ratings = ratings[0]\n",
    "press_movies_ratings = ratings[1]\n",
    "user_series_ratings = ratings[2]\n",
    "user_movies_ratings = ratings[3]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff1774c64d83fce3825259fa3771bbe70271854497325f5fa1e2c1b92279703"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
