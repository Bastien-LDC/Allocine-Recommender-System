{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎦**Web Scraping User Ratings From AlloCiné.fr**🏆⭐⭐⭐⭐⭐\n",
    "\n",
    "After building our movies and series dataframes in the previous notebooks (see [Webscraping Movies](https://github.com/Bastien-LDC/Allocine-Recommender-System/blob/master/Webscraping/Webscraping_Movies_From_AlloCine.ipynb) and [Webscraping Series](https://github.com/Bastien-LDC/Allocine-Recommender-System/blob/master/Webscraping/Webscraping_Series_From_AlloCine.ipynb)), this time we will retrieve the **user and press ratings** of movies and series from the website of AlloCiné. We will proceed as follows: \n",
    "- We reconstruct the comments section urls of the press and the spectators for each movies and/or series with the function `getCommentsUrl(...)`.\n",
    "- From there, we scrape the rating data from the press and/or user page for each movie and/or series with `ScrapeURL(...)`. \n",
    "- The user can be either a person or a press newspaper. Separated files are generated for each type of user, whether for movies or series.\n",
    "\n",
    "*📝Note 1: We use the popular BeautifulSoup package*\n",
    "\n",
    "## **Functions :**\n",
    "\n",
    "### `getCommentsUrl(movies_df, series_df, spect, press)` :\n",
    "\n",
    "This function will call two sub-functions: `getMoviesCommentsUrl(movies_df, spect, press)` and `getSeriesCommentsUrl(series_df, spect, press)`. Both functions respectively retrieve the list of movies and series ID in the movies and series dataframes generated by `getMoviesUrl(start_page, end_page, nb_pages)` and `getSeriesUrl(start_page, end_page, nb_pages)` in the previous scripts. Then, we use the IDs to reconstruct the comments section urls of the press and the spectators (users). We can chose to get the comments section from the user or the press for each video type.\n",
    "We then store the lists of urls in a csv file entitled `user_comments_url.csv` (resp. `press_comments_url.csv`), in both movies and series directory (`../Movies/Comments/` and `../Series/Comments/`).\n",
    "\n",
    "### `ScrapeURL(press_series_urls, press_movies_urls, user_series_urls, user_movies_urls, nb_users)` :\n",
    "\n",
    "This function iterates over the list of movies or series comments section urls generated by `getCommentsUrl(movies_df, series_df, spect, press)` and scrape the press and user rating data for each video type, by calling respectively `ScrapePressURL(series_urls, movies_urls)` and `ScrapeUserURL(series_urls, movies_urls)`. You can choose whether to scrape the press or the user ratings, whether for movies or series, or both, depending on the parameters value as input of the `ScrapeURL(...)` function.\n",
    "We use the press comments urls for movies and series to get the press ratings with `getPressRatings(press_soup)`.\n",
    "We use the user comments urls for movies and series to get the user ratings with `getUserRatings(user_rating_url, last_page, nb_users)`.\n",
    "\n",
    "In the process, we extract, if available :\n",
    "- `user_id`: AlloCiné user id (unavailable for the press)\n",
    "- `(user/press)_name`: AlloCiné user/press name\n",
    "- `(movie/series)_id`: AlloCiné movie/series id\n",
    "- `(user/press)_rating`: AlloCiné user/press rating (from 0.5 to 5 stars ⭐⭐⭐⭐⭐)\n",
    "- `date`: date of the rating (unavailable for the press)\n",
    "\n",
    "*📝Note 2: Intermediate functions were created to retrieve each individual feature of the dataframe, which makes it easy for debugging.*\n",
    "\n",
    "The function `ScrapeURL(...)` returns two objects : one list containing the 4 generated ratings dataframes (user_rating_movies, press_rating_movies, user_rating_series, press_rating_series), and another list of the respective errors (as dataframes) for those ratings. In addition, the eight dataframes are saved as `user_ratings_movies.csv` and `press_ratings_movies.csv` (respectively `user_ratings_series.csv` and `press_ratings_series.csv`), and as `press_ratings_errors` and `user_ratings_errors` (in both `../Movies/Ratings/` and `../Series/Ratings/`).\n",
    "\n",
    "\n",
    "(⚠️ **Warning** ⚠️: the process can take a while, depending on the number of movies or series and the number of user pages you choose to scrape. It is recommended to use a dedicated computer for this process, as it can take a long time to complete.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## **Import libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\AppData\\Local\\Temp\\ipykernel_11508\\334739753.py:17: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from time import time\n",
    "from time import sleep\n",
    "import re\n",
    "from datetime import timedelta, date\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import dateparser\n",
    "\n",
    "from warnings import warn, filterwarnings\n",
    "from IPython.core.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SUPPRIMER APRES TESTS\n",
    "response = urlopen(\"https://www.allocine.fr/series/ficheserie-18529/critiques/membres-critiques/\")\n",
    "html_text = response.read().decode(\"utf-8\")\n",
    "spect_html_soup = BeautifulSoup(html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## **Function:** Getting the comments section urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get Movie Comments Section: `getMoviesCommentsUrl(movies_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoviesCommentsUrl(movies_df: pd.DataFrame, spect: bool=False, press: bool=False) -> None:\n",
    "    '''\n",
    "    Get the comments section url for each movie\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param movies_df: DataFrame of movies\n",
    "    :param spect: Boolean, True if you want to get the spectators section\n",
    "    :param press: Boolean, True if you want to get the press section\n",
    "    :return: nothing but saves urls in csv files\n",
    "    '''    \n",
    "    # Get the list of movies_id from the movies_df\n",
    "    movies_id_list = movies_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_spect_requests, p_press_requests = 0, 0\n",
    "        \n",
    "    for v_id in movies_id_list:\n",
    "        if spect:\n",
    "            # Url for the spectators/user section sorted by the descending number of reviews per user\n",
    "            url_spect = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/spectateurs/membres-critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "            p_spect_requests += 1\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)    \n",
    "            p_press_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Movies/Comments/'\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(comments_path), exist_ok=True)\n",
    "    if spect:\n",
    "        print(f'--> Done; {p_spect_requests} Movies Users Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        print(f'--> Done; {p_press_requests} Movies Press Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get Series Comments Section: `getSeriesCommentsUrl(series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeriesCommentsUrl(series_df: pd.DataFrame, spect: bool=False, press: bool=False) -> None:\n",
    "    '''\n",
    "    Get the comments section url for each series\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param series_df: DataFrame of series\n",
    "    :param spect: Boolean, True if you want to get the spectators section\n",
    "    :param press: Boolean, True if you want to get the press section\n",
    "    :return: nothing but saves urls in csv files\n",
    "    '''\n",
    "    # Get the list of series_id from the series_df\n",
    "    series_id_list = series_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_spect_requests, p_press_requests = 0, 0\n",
    "        \n",
    "    for v_id in series_id_list:\n",
    "        if spect:\n",
    "            # Url for the spectators/user section sorted by the descending number of reviews per user\n",
    "            url_spect = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/membres-critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "            p_spect_requests += 1\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "            p_press_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Series/Comments/'\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(comments_path), exist_ok=True)\n",
    "    if spect:\n",
    "        print(f'--> Done; {p_spect_requests} Series Users Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        print(f'--> Done; {p_press_requests} Series Press Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Main function: `getCommentsUrl(movies_df, series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommentsUrl(movies_df: pd.DataFrame=None, series_df: pd.DataFrame=None, spect: bool=False, press: bool=False) -> None:\n",
    "    '''\n",
    "    Get the comments section url for each movie and series\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param movies_df: DataFrame of movies\n",
    "    :param series_df: DataFrame of series\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in a csv file\n",
    "    '''\n",
    "    try:\n",
    "        if movies_df is not None:\n",
    "            getMoviesCommentsUrl(movies_df, spect, press)\n",
    "        if series_df is not None:\n",
    "            getSeriesCommentsUrl(series_df, spect, press)\n",
    "    except:\n",
    "        print('Error in getCommentsUrl function!')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Functions:** Getting ratings infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ID: `get_ID(url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ID(url: str) -> int:\n",
    "    '''\n",
    "    Get the movie or serie ID from an url.\n",
    "    :param url: url containing the movie or serie ID\n",
    "    :return: The movie or serie ID.'''\n",
    "    id = int(re.sub(r\"\\D\", \"\", url))\n",
    "    return id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get url: `get_url(url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url: str):\n",
    "    '''\n",
    "    Try to get and open the page url.\n",
    "    :param url: url to get the content from.\n",
    "    :return: the content of the url\n",
    "    '''\n",
    "    page = ''\n",
    "    while page == '':\n",
    "        try:\n",
    "            page = urlopen(url)\n",
    "            break\n",
    "        except:\n",
    "            print(f\"Error occured when opening the url.\\nURL: {url}\")\n",
    "            print(\"\\nConnection refused by the server...\")\n",
    "            print(\"Let's wait for 5 seconds\")\n",
    "            print(\"ZZzzzz...\")\n",
    "            time.sleep(5)\n",
    "            print(\"Ok, now let's try again...\\n\")\n",
    "            continue\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Convert French months to English months: `convert_month(month)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_month(month: str) -> str:\n",
    "    '''\n",
    "    Convert French months into English months for the dateparser to work.\n",
    "    :param month: French month.\n",
    "    :return: English month.'''\n",
    "    if month == \"janvier\":\n",
    "        return \"January\"\n",
    "    elif month == \"février\":\n",
    "        return \"February\"\n",
    "    elif month == \"mars\":\n",
    "        return \"March\"\n",
    "    elif month == \"avril\":\n",
    "        return \"April\"\n",
    "    elif month == \"mai\":\n",
    "        return \"May\"\n",
    "    elif month == \"juin\":\n",
    "        return \"June\"\n",
    "    elif month == \"juillet\":\n",
    "        return \"July\"\n",
    "    elif month == \"août\":\n",
    "        return \"August\"\n",
    "    elif month == \"septembre\":\n",
    "        return \"September\"\n",
    "    elif month == \"octobre\":\n",
    "        return \"October\"\n",
    "    elif month == \"novembre\":\n",
    "        return \"November\"\n",
    "    elif month == \"décembre\":\n",
    "        return \"December\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Convert text to rating: `convertTextToRating(text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_rating(text: str) -> float:\n",
    "    '''\n",
    "    Convert the press evaluation text into a rating.\n",
    "    :param text: Evaluation of the movie as a string \n",
    "    :return: corresponding float rating value\n",
    "    '''\n",
    "    if text==\"Nul\":\n",
    "        return 0.5\n",
    "    elif text==\"Très mauvais\":\n",
    "        return 1.0\n",
    "    elif text==\"Mauvais\":\n",
    "        return 1.5\n",
    "    elif text==\"Pas terrible\":\n",
    "        return 2.0\n",
    "    elif text==\"Moyen\":\n",
    "        return 2.5  \n",
    "    elif text==\"Pas mal\":\n",
    "        return 3.0\n",
    "    elif text==\"Bien\":\n",
    "        return 3.5\n",
    "    elif text==\"Très bien\":\n",
    "        return 4.0\n",
    "    elif text==\"Excellent\":\n",
    "        return 4.5\n",
    "    elif text==\"Chef-d'oeuvre\":\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get press-ratings: `getPressRatings(press_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPressRatings(press_soup: BeautifulSoup) -> dict[str, float]:\n",
    "    '''\n",
    "    Get press_ratings from the comments section url for each movie or series.\n",
    "    :param press_soup: BeautifulSoup object of the press comments section url\n",
    "    :return: a dictionary of ratings (key: press name, value: press rating) if the ratings are available, else None.\n",
    "    '''\n",
    "    press_ratings = {}\n",
    "    div_ratings = press_soup.find_all('li', {'class': 'item'})\n",
    "    if div_ratings:\n",
    "        press_ratings = {div.text.strip():convert_text_to_rating(div.find('span')['title']) for div in div_ratings}\n",
    "        return press_ratings\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get user-ratings: `getUserRatings(user_rating_url, last_page, nb_users)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserRatings(user_rating_url: str, last_page: int=1, nb_users: int=5) -> dict[tuple[str,str]: tuple[float,date]]:\n",
    "    '''\n",
    "    Get user_ratings from the comments section url for each movie or series.\n",
    "    We will keep only the nb_users first users who have posted the most reviews.\n",
    "    :param user_rating_url : url of the user ratings section.\n",
    "    :param last_page: last page of the user ratings section (default 1).\n",
    "    :param nb_users: Number of users to keep (default 5).\n",
    "    :return: a dictionary of ratings {key=(user_id, user_name): value=(user rating, date)} if the ratings are available, else None.\n",
    "    '''\n",
    "    user_ratings = {}\n",
    "    for n_page in range(1,last_page+1):\n",
    "        # If the number of users is reached, stop the loop.\n",
    "        # Else, get the next page of the user ratings section until the last page is reached.\n",
    "        if len(user_ratings) == nb_users:       \n",
    "            break\n",
    "\n",
    "        user_rating_url_page = user_rating_url + f'?page={n_page}'        \n",
    "        try :\n",
    "            response = get_url(user_rating_url_page)\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            html_text = response.read().decode(\"utf-8\")\n",
    "            user_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "            # Collect only the reviews infos from the page in a list\n",
    "            div_ratings = user_soup.find_all('div', {'class': 'review-card'})\n",
    "            if div_ratings:\n",
    "                for item in div_ratings:\n",
    "                    # If the number of users is reached, stop the loop.\n",
    "                    if len(user_ratings) == nb_users:       \n",
    "                        break   \n",
    "                    # If the user is not a visitor...\n",
    "                    if item.find('span', {'class': 'item-profil'}):\n",
    "                        # ...Get the user id, name, rating and date of the rating   \n",
    "                        user_id = item.find('span', {'class': 'item-profil'})['data-targetuserid']\n",
    "                        user_name = item.span['title']\n",
    "                        user_rating = float(re.sub(\",\", \".\", item.find(\"span\", {\"class\": \"stareval-note\"}).text))\n",
    "                        user_date = item.find('span', {'class': 'review-card-meta-date'}).text.strip().replace('Publiée le ', '')\n",
    "                        # Convert the date from French to English\n",
    "                        month = user_date.split(' ')[1]\n",
    "                        user_date = dateparser.parse(user_date.replace(month, convert_month(month)), date_formats=[\"%d %B %Y\"]).date()\n",
    "                        # Append to the dictionary\n",
    "                        user_ratings[(user_id, user_name)] = (user_rating, user_date)\n",
    "        except:\n",
    "            print(f'Error in getUserRatings function!')\n",
    "            traceback.print_exc()\n",
    "    return user_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Function:** Scraping the ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Scraping Press URLs: `ScrapePressURL(series_urls, movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapePressURL(series_url: list=None, movies_url: list=None) -> tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame]:   \n",
    "    '''\n",
    "    Scrape the press_ratings from the series and/or movies press comments section urls (if available).\n",
    "    :param series_url: list of series press comments section urls.\n",
    "    :param movies_url: list of movies press comments section urls.\n",
    "    :return: dataframes of series and movies press ratings and the errors.\n",
    "    '''\n",
    "    # init the dataframes    \n",
    "    df_series, df_movies, errors_series, errors_movies = None, None, None, None\n",
    "    if series_url is None and movies_url is None:\n",
    "        print('No url to scrape!')\n",
    "        return df_series, df_movies, errors_series, errors_movies\n",
    "    if series_url is not None:\n",
    "        c = [\"press_name\",\n",
    "        \"series_id\",\n",
    "        \"press_rating\",]\n",
    "        df_series = pd.DataFrame(columns=c)\n",
    "    if movies_url is not None:\n",
    "        c = [\"press_name\",\n",
    "        \"movie_id\",\n",
    "        \"press_rating\",]\n",
    "        df_movies = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    ns_request = 0\n",
    "    nm_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors_series = []\n",
    "    errors_movies = []\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Scraping the series press ratings\n",
    "    # -----------------------------------\n",
    "    if series_url is not None:\n",
    "        # request loop\n",
    "        clear_output(wait=True)\n",
    "        print(\"------ Scraping series press ratings ------\\n\")\n",
    "        # Monitoring with tqdm_notebook() progress bar\n",
    "        for url in tqdm(series_url,desc='Fetching Series Press Ratings'):\n",
    "            try :\n",
    "                response = get_url(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                ns_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                # print(f'Series Press Request #{ns_request}; Frequency: {ns_request/elapsed_time} requests/s')\n",
    "                # clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Series Press Request #{}; Status code: {}'.format(ns_request, response.status_code))\n",
    "                    errors_series.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the series_id from the url\n",
    "                series_id = get_ID(url)            \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_series_tmp = pd.DataFrame({'press_name': [id],\n",
    "                                        'series_id': [series_id],\n",
    "                                        'press_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_series = pd.concat([df_series, df_series_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_series.append(url)\n",
    "                warn(f'Series Press Request #{ns_request} fail; Press rating does not exist! Total errors : {len(errors_series)}')\n",
    "                #traceback.print_exc()\n",
    "\n",
    "        # Monitoring \n",
    "        series_path = '../Series/Ratings/'\n",
    "        # We create the folder if not exists\n",
    "        os.makedirs(os.path.dirname(series_path), exist_ok=True)\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'--> Done; {ns_request} Series Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_series)} errors (series with no press ratings)')\n",
    "        # Saving files\n",
    "        df_series.to_csv(f\"{series_path}press_ratings_series.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_series = pd.DataFrame(errors_series, columns=['url'])\n",
    "        errors_df_series.to_csv(f\"{series_path}press_ratings_errors.csv\",index=False,header=False)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # Scraping the movies press ratings\n",
    "    # -----------------------------------\n",
    "    if movies_url is not None:\n",
    "        # request loop\n",
    "        clear_output(wait=True)\n",
    "        print(\"------ Scraping movies press ratings ------\\n\")\n",
    "        # Monitoring with tqdm_notebook() progress bar\n",
    "        for url in tqdm(movies_url,desc='Fetching Movies Press Ratings'):\n",
    "            try :\n",
    "                response = get_url(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                nm_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                #print(f'Movie Press Request #{nm_request}; Frequency: {nm_request/elapsed_time} requests/s')\n",
    "                #clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Movie Press Request #{}; Status code: {}'.format(nm_request, response.status_code))\n",
    "                    errors_movies.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the movie_id from the url\n",
    "                movie_id = get_ID(url)           \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_movies_tmp = pd.DataFrame({'press_name': [id],\n",
    "                                        'movie_id': [movie_id],\n",
    "                                        'press_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_movies = pd.concat([df_movies, df_movies_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_movies.append(url)\n",
    "                warn(f'Movie Press Request #{nm_request} fail; Press rating does not exist! Total errors : {len(errors_movies)}')\n",
    "                #traceback.print_exc()\n",
    "            \n",
    "        # Monitoring\n",
    "        movies_path = '../Movies/Ratings/'\n",
    "        # We create the folder if not exists\n",
    "        os.makedirs(os.path.dirname(movies_path), exist_ok=True)\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'--> Done; {nm_request} Movies Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_movies)} errors (movies with no press ratings)')\n",
    "        # Saving files\n",
    "        df_movies.to_csv(f\"{movies_path}press_ratings_movies.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_movies = pd.DataFrame(errors_movies, columns=['url'])\n",
    "        errors_df_movies.to_csv(f\"{movies_path}press_ratings_errors.csv\",index=False,header=False)\n",
    "    \n",
    "    # return dataframe and errors\n",
    "    return df_series, df_movies, errors_series, errors_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Scraping User URLs: `ScrapeUserURL(series_urls, movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeUserURL(series_url: list=None, movies_url: list=None, nb_users: int=5) -> tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "    '''\n",
    "    Scrape the user_ratings from the series and/or movies press comments section urls (if available).\n",
    "    :param series_url: list of series user comments urls\n",
    "    :param movies_url: list of movies user comments urls\n",
    "    :param nb_users: number of users to scrape (default: 5)\n",
    "    :return: dataframes of series and movies user ratings and the errors.  \n",
    "    '''\n",
    "    # init the dataframes    \n",
    "    df_series, df_movies, errors_series, errors_movies = None, None, None, None\n",
    "    if series_url is None and movies_url is None:\n",
    "        print('No url to scrape!')\n",
    "        return df_series, df_movies, errors_series, errors_movies\n",
    "    if series_url is not None:\n",
    "        c = [\"user_id\",\n",
    "        \"user_name\",\n",
    "        \"series_id\",\n",
    "        \"user_rating\",\n",
    "        \"date\",\n",
    "    ]\n",
    "        df_series = pd.DataFrame(columns=c)\n",
    "    if movies_url is not None:\n",
    "        c = [\"user_id\",\n",
    "        \"user_name\",\n",
    "        \"movie_id\",\n",
    "        \"user_rating\",\n",
    "        \"date\",\n",
    "    ]\n",
    "        df_movies = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    ns_request = 0\n",
    "    nm_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors_series = []\n",
    "    errors_movies = []\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Scraping the series user ratings\n",
    "    # -----------------------------------\n",
    "    if series_url is not None:\n",
    "        # request loop\n",
    "        clear_output(wait=True)\n",
    "        print(\"------ Scraping series user ratings ------\\n\")\n",
    "        # Monitoring with tqdm_notebook() progress bar\n",
    "        for url in tqdm(series_url,desc='Fetching Series User Ratings'):\n",
    "            try :\n",
    "                response = get_url(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                ns_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                #print(f'Series User Request #{ns_request}; Frequency: {ns_request/elapsed_time} requests/s')\n",
    "                #clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Series User Request #{}; Status code: {}'.format(ns_request, response.status_code))\n",
    "                    errors_series.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                user_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the series_id from the url\n",
    "                series_id = get_ID(url)\n",
    "                # Get the url last page if exists. Default 1.\n",
    "                last_page = int(user_html_soup.find_all('span', {'class': 'button-md'})[-1].text) if user_html_soup.find_all('span', {'class': 'button-md'}) else 1\n",
    "                # Get the user_ratings\n",
    "                user_ratings = getUserRatings(url,last_page,nb_users=nb_users)\n",
    "                \n",
    "                for id, rating in user_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_series_tmp = pd.DataFrame({'user_id': [id[0]],\n",
    "                                        'user_name': [id[1]],\n",
    "                                        'series_id': [series_id],\n",
    "                                        'user_rating': [rating[0]],\n",
    "                                        'date': [rating[1]]\n",
    "                                        })                                    \n",
    "                    df_series = pd.concat([df_series, df_series_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_series.append(url)\n",
    "                warn(f'Series User Request #{ns_request} fail; User rating does not exist! Total errors : {len(errors_series)}')\n",
    "                #traceback.print_exc()\n",
    "        \n",
    "        # Monitoring\n",
    "        series_path = '../Series/Ratings/'\n",
    "        # We create the folder if not exists\n",
    "        os.makedirs(os.path.dirname(series_path), exist_ok=True)\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'--> Done; {ns_request} Series User Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_series)} errors (series with no user ratings)')\n",
    "        # Saving files\n",
    "        df_series.to_csv(f\"{series_path}user_ratings_series.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_series = pd.DataFrame(errors_series, columns=['url'])\n",
    "        errors_df_series.to_csv(f\"{series_path}user_ratings_errors.csv\",index=False,header=False)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # Scraping the movies user ratings\n",
    "    # -----------------------------------\n",
    "    if movies_url is not None:\n",
    "        # request loop\n",
    "        clear_output(wait=True)\n",
    "        print(\"------ Scraping movies user ratings ------\\n\")\n",
    "        # Monitoring with tqdm_notebook() progress bar\n",
    "        for url in tqdm(movies_url,desc='Fetching Movies User Ratings'):\n",
    "            try :\n",
    "                response = get_url(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                nm_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                #print(f'Movie User Request #{nm_request}; Frequency: {nm_request/elapsed_time} requests/s')\n",
    "                #clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Movie User Request #{}; Status code: {}'.format(nm_request, response.status_code))\n",
    "                    errors_movies.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                user_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the movie_id from the url\n",
    "                movie_id = get_ID(url)          \n",
    "                # Get the url last page if exists. Default 1.\n",
    "                last_page = int(user_html_soup.find_all('span', {'class': 'button-md'})[-1].text) if user_html_soup.find_all('span', {'class': 'button-md'}) else 1\n",
    "                # Get the user_ratings\n",
    "                user_ratings = getUserRatings(url,last_page,nb_users=nb_users)\n",
    "                \n",
    "                for id, rating in user_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_movies_tmp = pd.DataFrame({'user_id': [id[0]],\n",
    "                                        'user_name': [id[1]],\n",
    "                                        'movie_id': [movie_id],\n",
    "                                        'user_rating': [rating[0]],\n",
    "                                        'date': [rating[1]]\n",
    "                                        })                                    \n",
    "                    df_movies = pd.concat([df_movies, df_movies_tmp], ignore_index=True)                             \n",
    "            except:\n",
    "                errors_movies.append(url)\n",
    "                warn(f'Movie User Request #{nm_request} fail; User rating does not exist! Total errors : {len(errors_movies)}')\n",
    "                #traceback.print_exc()\n",
    "            \n",
    "        # Monitoring\n",
    "        movies_path = '../Movies/Ratings/'\n",
    "        # We create the folder if not exists\n",
    "        os.makedirs(os.path.dirname(movies_path), exist_ok=True) \n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'--> Done; {nm_request} Movies User Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_movies)} errors (movies with no user ratings)')\n",
    "        # Saving files\n",
    "        df_movies.to_csv(f\"{movies_path}user_ratings_movies.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_movies = pd.DataFrame(errors_movies, columns=['url'])\n",
    "        errors_df_movies.to_csv(f\"{movies_path}user_ratings_errors.csv\",index=False,header=False)\n",
    "    \n",
    "    # return dataframe and errors\n",
    "    return df_series, df_movies, errors_series, errors_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Main function: `ScrapeURL(press_series_urls, press_movies_urls, user_series_urls, user_movies_urls, nb_users)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeURL(press_series_urls: list=None, press_movies_urls: list=None, user_series_urls: list=None, user_movies_urls: list=None, nb_users: int=5) -> tuple[list,list]:\n",
    "    '''\n",
    "    Scrape the user and press ratings from the movie and series pages.\n",
    "    :param press_series_urls: list of press series urls\n",
    "    :param press_movies_urls: list of press movies urls\n",
    "    :param user_series_urls: list of user series urls\n",
    "    :param user_movies_urls: list of user movies urls\n",
    "    :return: 2 lists of dataframes of user and press: one for the ratings \n",
    "    and the other one for the errors.\n",
    "    '''\n",
    "    # We ignore dateparse warnings\n",
    "    filterwarnings(\"ignore\",message=\"The localize method is no longer necessary, as this time zone supports the fold attribute\")\n",
    "\n",
    "    # Monitoring the press scraping\n",
    "    start_time_press = time()    \n",
    "    # Init the ratings and errors lists\n",
    "    ratings,errors = [],[]\n",
    "    # Scraping the press ratings\n",
    "    press_series_ratings, press_movies_ratings, press_errors_series, press_errors_movies = ScrapePressURL(press_series_urls, press_movies_urls)\n",
    "    elapsed_time_press = time() - start_time_press\n",
    "    delta_press = timedelta(seconds=elapsed_time_press)\n",
    "    ratings.append(press_series_ratings)\n",
    "    ratings.append(press_movies_ratings)\n",
    "    errors.append(press_errors_series)\n",
    "    errors.append(press_errors_movies)   \n",
    "    # Monitoring the users scraping\n",
    "    start_time_users = time() \n",
    "    # Scraping the user ratings\n",
    "    user_series_ratings, user_movies_ratings, user_errors_series, user_errors_movies = ScrapeUserURL(user_series_urls, user_movies_urls, nb_users=nb_users)\n",
    "    elapsed_time_users = time() - start_time_users\n",
    "    delta_users = timedelta(seconds=elapsed_time_users)\n",
    "    ratings.append(user_series_ratings)\n",
    "    ratings.append(user_movies_ratings)\n",
    "    errors.append(user_errors_series)\n",
    "    errors.append(user_errors_movies)\n",
    "    # Monitoring the results\n",
    "    print(\"\\n    --- RECAP ---\")\n",
    "    if press_series_ratings is not None and press_errors_series is not None:\n",
    "        print(f'--> Done; {len(press_series_ratings)} Series Press Ratings requests in {delta_press} with {len(press_errors_series)} errors (series with no press ratings)')\n",
    "    if press_movies_ratings is not None and press_errors_movies is not None:\n",
    "        print(f'--> Done; {len(press_movies_ratings)} Movies Press Ratings requests in {delta_press} with {len(press_errors_movies)} errors (movies with no press ratings)')\n",
    "    if user_series_ratings is not None and user_errors_series is not None:\n",
    "        print(f'--> Done; {len(user_series_ratings)} Series User Ratings requests in {delta_users} with {len(user_errors_series)} errors (series with no user ratings)')\n",
    "    if user_movies_ratings is not None and user_errors_movies is not None:\n",
    "        print(f'--> Done; {len(user_movies_ratings)} Movies User Ratings requests in {delta_users} with {len(user_errors_movies)} errors (movies with no user ratings)')\n",
    "    return ratings, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## **Launching the script**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the movies and series dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movies and series dataframes\n",
    "def loadDataFrames(nrows: int=None) -> tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    '''\n",
    "    Load the movies and series dataframes\n",
    "    :param nrows: number of rows to load\n",
    "    :return: movies and series dataframes\n",
    "    '''\n",
    "    movies_df = pd.read_csv('../Movies/Data/allocine_movies_100p.csv', nrows=nrows)\n",
    "    series_df = pd.read_csv('../Series/Data/allocine_series_100p.csv', nrows=nrows)\n",
    "    return movies_df, series_df\n",
    "movies_df, series_df = loadDataFrames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting the comments section urls from spectators and press for movies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 1314 Movies Users Comments Page Requests in 0:00:00.001252\n",
      "--> Done; 1314 Movies Press Comments Page Requests in 0:00:00.006182\n",
      "--> Done; 1417 Series Users Comments Page Requests in 0:00:00.001018\n",
      "--> Done; 1417 Series Press Comments Page Requests in 0:00:00.011786\n"
     ]
    }
   ],
   "source": [
    "getCommentsUrl(movies_df=movies_df, series_df=series_df, spect=True, press=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the comments section urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 1314 Movies Spectators URLs succesfully loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadingURL(movies: bool=False, series: bool=False, spect: bool=False, press: bool=False) -> tuple[list,list,list,list]:\n",
    "    '''\n",
    "    Load the urls of the movies and series pages.\n",
    "    :param movies: Boolean, if True, load the movies urls\n",
    "    :param series: Boolean, if True, load the series urls\n",
    "    :param spect: Boolean, if True, load the spectator urls for movies and/or series\n",
    "    :param press: Boolean, if True, load the press urls for movies and/or series\n",
    "    :return: urls lists of the movies and series press and user rating pages    \n",
    "    '''\n",
    "    user_series_urls, user_movies_urls, press_series_urls, press_movies_urls =  None, None, None, None\n",
    "    if movies:\n",
    "        if spect:\n",
    "            user_movies_urls = pd.read_csv(\"../Movies/Comments/user_comments_urls.csv\",names=['url'])['url'].tolist()\n",
    "            print(f\"--> {len(user_movies_urls)} Movies Spectators URLs succesfully loaded!\")\n",
    "        if press:\n",
    "            press_movies_urls = pd.read_csv(\"../Movies/Comments/press_comments_urls.csv\",names=['url'])['url'].tolist()\n",
    "            print(f\"--> {len(press_movies_urls)} Movies Press URLs succesfully loaded!\")\n",
    "    if series:\n",
    "        if spect:\n",
    "            user_series_urls = pd.read_csv(\"../Series/Comments/user_comments_urls.csv\",names=['url'])['url'].tolist()\n",
    "            print(f\"--> {len(user_series_urls)} Series Spectators URLs succesfully loaded!\")\n",
    "        if press:\n",
    "            press_series_urls = pd.read_csv(\"../Series/Comments/press_comments_urls.csv\",names=['url'])['url'].tolist()\n",
    "            print(f\"--> {len(press_series_urls)} Series Press URLs succesfully loaded!\")\n",
    "    return user_series_urls, user_movies_urls, press_series_urls, press_movies_urls\n",
    "\n",
    "user_series_urls, user_movies_urls, press_series_urls, press_movies_urls = loadingURL(movies=True, series=False, spect=True, press=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Scraping movies user ratings ------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8395f06b1cda4232982c8c6c6323b01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching Movies User Ratings:   0%|          | 0/1314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 1314 Movies User Ratings requests in 6:12:28.403621 with 0 errors (movies with no user ratings)\n",
      "\n",
      "    --- RECAP ---\n",
      "--> Done; 105711 Movies User Ratings requests in 6:12:28.812086 with 0 errors (movies with no user ratings)\n"
     ]
    }
   ],
   "source": [
    "# Scraping the user and/or press ratings from series and/or movies\n",
    "ratings, errors = ScrapeURL(press_series_urls=press_series_urls, \n",
    "                            press_movies_urls=press_movies_urls, \n",
    "                            user_series_urls=user_series_urls, \n",
    "                            user_movies_urls=user_movies_urls,\n",
    "                            nb_users=100,\n",
    "                            )\n",
    "press_series_ratings = ratings[0]\n",
    "press_movies_ratings = ratings[1]\n",
    "user_series_ratings = ratings[2]\n",
    "user_movies_ratings = ratings[3]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff1774c64d83fce3825259fa3771bbe70271854497325f5fa1e2c1b92279703"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
