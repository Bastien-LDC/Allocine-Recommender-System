{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Series Data From AlloCiné.fr\n",
    "\n",
    "This script builds a DataFrame by web scraping the series data from AlloCiné — a company which provides information on French cinema. Because of the long delay, we choose to scrape the data in two steps : \n",
    "- First we scrape the url of each series with `getSeriesUrl()`\n",
    "- Lastly we use the url list to scrape the data for each series with `ScrapeURL()`\n",
    "\n",
    "*Note : We use the popular BeautifulSoup package*\n",
    "\n",
    "## Functions :\n",
    "\n",
    "### `getSeriesUrl(start_page, end_page)` :\n",
    "\n",
    "Save a CSV files of the url list as `series_url.csv`. The argument must be integers and are used to select the range of page you want to scrape the data from. The `end_page` is included.\n",
    "\n",
    "### `ScrapeURL(series_url)` :\n",
    "\n",
    "Iterate over the list of url generated by `getSeriesUrl()` and scrape the data for each series. In the process, we extract :\n",
    "\n",
    "- `id` : Allocine series id\n",
    "- `title` : the series' title (in french)\n",
    "- `status` : the series' status (in french)\n",
    "- `release_date`: the release date\n",
    "- `duration`: the series average episode length (in minutes)\n",
    "- `nb_seasons`: the number of seasons\n",
    "- `nb_episodes`: the number of episodes\n",
    "- `genres` : the series types (as an array, up to three different types)\n",
    "- `directors` : series directors (as an array)\n",
    "- `actors` : main actors of the series (as an array)\n",
    "- `nationality`: nationality of the series (as an array)\n",
    "- `press_rating`: press ratings (from 0.5 to 5 stars)\n",
    "- `nb_press_rating`: number of press votes\n",
    "- `spec_rating`:  AlloCiné users ratings (from 0.5 to 5 stars)\n",
    "- `nb_spec_rating`: number of users votes\n",
    "- `summary`: short summary of the series in french\n",
    "- `poster`: url of the series' poster\n",
    "\n",
    "The function `ScrapeURL()` returns two objects : the data as a dataframe and the url list of error as a list. In addition the two objects are saved as `allocine_series.csv` and `allocine_errors.csv`. You could pass the list of errors into `ScrapeURL()` to get the extra data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\AppData\\Local\\Temp\\ipykernel_14360\\418001640.py:14: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from time import time\n",
    "from time import sleep\n",
    "from datetime import timedelta\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `getSeriesUrl(start_page, end_page)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeriesUrl(start_page, end_page=None, nb_pages=1):\n",
    "    '''\n",
    "    Scrape the series urls from the AlloCine website's series page (http://www.allocine.fr/series-tv/).\n",
    "    :param start_page: The first page to scrape.\n",
    "    :param end_page: The last page to scrape (included) (optional).\n",
    "    :param nb_pages: The number of pages to scrape (default 1).\n",
    "    :return:  Nothing but saves the list of series urls in a csv file.'''\n",
    "    if start_page <= 0:\n",
    "        raise ValueError('start_page must be positive !')\n",
    "\n",
    "    # Set the list\n",
    "    series_url = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = start_page\n",
    "    if nb_pages < 1:\n",
    "        nb_pages = 1\n",
    "    if end_page == None or end_page < start_page:\n",
    "        end_page = start_page + nb_pages - 1\n",
    "    \n",
    "    s_requests = 0\n",
    "        \n",
    "    for p in range(start_page, end_page + 1):\n",
    "\n",
    "        # Get request\n",
    "        url = f'https://www.allocine.fr/series-tv/?page={p}'\n",
    "        response = urlopen(url)\n",
    "        \n",
    "        # Pause the loop\n",
    "        sleep(randint(1,2))\n",
    "            \n",
    "        # Monitoring the requests\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'>Page Request: {p_requests}; Frequency: {p_requests/elapsed_time} requests/s')\n",
    "        clear_output(wait = True)\n",
    "            \n",
    "        # Warning for non-200 status codes\n",
    "        if response.status != 200:\n",
    "            warn(f'>Page Request: {p_requests}; Status code: {response.status_code}')\n",
    "\n",
    "        # Break the loop if the number of requests is greater than expected\n",
    "        if p_requests > end_page:\n",
    "            warn('Number of requests was greater than expected.')\n",
    "            break\n",
    "\n",
    "        # Parse the content of the request with BeautifulSoup\n",
    "        html_text = response.read().decode(\"utf-8\")\n",
    "        html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "        # Select all the series url from a single page\n",
    "        series = html_soup.find_all('h2', 'meta-title')\n",
    "        s_requests += len(series)\n",
    "       \n",
    "        # Monitoring the requests\n",
    "        print(f'>Page Request: {p_requests}; Series Request: {s_requests}')\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        # Pause the loop\n",
    "        sleep(1)\n",
    "        \n",
    "        for serie in series:\n",
    "            series_url.append(f'http://www.allocine.fr{serie.a[\"href\"]}')                 \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    series_path = '../Series/Data/'\n",
    "    os.makedirs(os.path.dirname(series_path), exist_ok=True) #create folders if not exists\n",
    "    print(f'--> Done; {p_requests-1} Page Requests and {s_requests} Series Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    r = np.asarray(series_url)\n",
    "    np.savetxt(f\"{series_path}series_url.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Getting series infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SUPPRIMER APRES TESTS\n",
    "response = urlopen(\"https://www.allocine.fr/series/ficheserie_gen_cserie=29127.html\")\n",
    "response = urlopen(\"https://www.allocine.fr/series/ficheserie_gen_cserie=28329.html\")\n",
    "html_text = response.read().decode(\"utf-8\")\n",
    "series_html_soup = BeautifulSoup(html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series ID: `get_series_ID(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_ID(series_soup):\n",
    "    '''\n",
    "    Scrape the series ID from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series ID.'''\n",
    "    series_ID = re.sub(\n",
    "        r\"\\D\", \"\", series_soup.find(\"nav\", {\"class\": \"third-nav\"}).a[\"href\"]\n",
    "        )\n",
    "    return series_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series title: `get_series_title(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_title(series_soup):\n",
    "    '''\n",
    "    Scrape the series title from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series title.'''\n",
    "    series_title = series_soup.find(\"div\", {\"class\": \"titlebar-title\"}).text.strip()\n",
    "    return series_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series status: `get_series_status(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_status(series_soup):\n",
    "    '''\n",
    "    Scrape the series status from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series status (En cours | Terminée | Annulée).'''\n",
    "    series_status = series_soup.find(\"div\", {\"class\": \"label-status\"}).text.strip()\n",
    "    return series_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series release date: `get_series_release_date(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_release_date(series_soup):\n",
    "    '''\n",
    "    Scrape the series release date from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series release date.'''\n",
    "    series_release_date = series_soup.find(\"div\", {\"class\": \"meta-body-item meta-body-info\"})\n",
    "    if series_release_date:\n",
    "        series_release_date = series_release_date.text.strip().split(\"\\n\")[0]\n",
    "        if get_series_status(series_soup) == \"Terminée\":\n",
    "            series_release_date = series_release_date.replace(\" \", \"\")      \n",
    "    return series_release_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series duration: `get_series_duration(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_duration(series_soup):\n",
    "    '''\n",
    "    Scrape the series duration from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series duration in minutes.'''\n",
    "    series_duration = series_soup.find(\"span\", {\"class\": \"spacer\"}).next_sibling.strip()\n",
    "    if series_duration != \"\":\n",
    "        duration_timedelta = pd.to_timedelta(series_duration).components\n",
    "        series_duration = duration_timedelta.hours * 60 + duration_timedelta.minutes\n",
    "    return series_duration    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series nb of seasons and episodes: `get_series_nb_seasons_episodes(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_nb_seasons_episodes(series_soup):\n",
    "    '''\n",
    "    Scrape the series number of seasons and episodes from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series number of seasons and episodes. None if not found.'''\n",
    "    series_nb_seasons_episodes = series_soup.find_all(\"div\", {\"class\": \"stats-numbers-row-item\"})\n",
    "    if series_nb_seasons_episodes:\n",
    "        series_nb_seasons = series_nb_seasons_episodes[0].div.text\n",
    "        series_nb_episodes = series_nb_seasons_episodes[1].div.text\n",
    "        return series_nb_seasons, series_nb_episodes\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series genres: `get_series_genres(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_genres(series_soup):\n",
    "    '''\n",
    "    Scrape the series genres from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series genres. None if not found.'''\n",
    "    div_genres = series_soup.find(\"div\", {\"class\": \"meta-body-item meta-body-info\"})\n",
    "    if div_genres:\n",
    "        series_genres = [\n",
    "            genre.text\n",
    "            for genre in div_genres.find_all(\"span\")\n",
    "            if \"\\n\" and '/' not in genre.text\n",
    "        ]\n",
    "        return \", \".join(series_genres)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series directors: `get_series_directors(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_directors(series_soup):\n",
    "    '''\n",
    "    Scrape the series directors from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series directors. None if not found.'''\n",
    "    div_directors = series_soup.find_all(\n",
    "            \"div\", {\"class\": \"meta-body-item meta-body-direction\"}\n",
    "        )\n",
    "    if div_directors:\n",
    "        series_directors = [\n",
    "            link.text\n",
    "            for directors in div_directors\n",
    "            for link in directors.find_all(\n",
    "                [\"a\", \"span\"], class_=re.compile(r\".*blue-link$\")\n",
    "            )\n",
    "        ]\n",
    "        return \", \".join(set(series_directors))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series actors: `get_series_actors(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_actors(series_soup):\n",
    "    '''\n",
    "    Scrape the series actors from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series actors. None if not found.'''\n",
    "    div_actors = series_soup.find(\"div\", {\"class\": \"meta-body-item meta-body-actor\"})\n",
    "    if div_actors:\n",
    "        series_actors = [actor.text for actor in div_actors.find_all([\"a\", \"span\"])][1:]\n",
    "        return \", \".join(series_actors)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series nationality: `get_series_nationality(series_soup)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_nationality(series_soup):\n",
    "    '''\n",
    "    Scrape the series nationality from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series nationality.'''\n",
    "    series_nationality = [\n",
    "            nationality.text.strip()\n",
    "            for nationality in series_soup.find_all(\"span\", class_=\"nationality\")\n",
    "        ]\n",
    "    return \", \".join(series_nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series press ratings: `get_series_press_rating(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_press_rating(series_soup):\n",
    "    '''\n",
    "    Scrape the series average press rating from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series average press rating. None if not found.'''\n",
    "    series_ratings = series_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in series_ratings:\n",
    "        if \"Presse\" in ratings.text:\n",
    "            return float(\n",
    "                re.sub(\n",
    "                    \",\", \".\", ratings.find(\"span\", {\"class\": \"stareval-note\"}).text\n",
    "                )\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series number of press ratings: `get_series_press_rating_count(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_press_rating_count(series_soup):\n",
    "    '''\n",
    "    Scrape the series number of press ratings from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series number of press ratings. None if not found.'''\n",
    "    series_ratings = series_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in series_ratings:\n",
    "        if \"Presse\" in ratings.text:\n",
    "            return float(\n",
    "                re.match(\n",
    "                    r\"\\s\\d+\",\n",
    "                    ratings.find(\"span\", {\"class\": \"stareval-review\"}).text,\n",
    "                ).group()\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series spectator ratings: `get_series_spec_rating(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_spec_rating(series_soup):\n",
    "    '''\n",
    "    Scrape the series average spectator rating from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series average spectator rating. None if not found.'''\n",
    "    series_ratings = series_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in series_ratings:\n",
    "        if \"Spectateurs\" in ratings.text:\n",
    "            return float(\n",
    "                re.sub(\n",
    "                    \",\", \".\", ratings.find(\"span\", {\"class\": \"stareval-note\"}).text\n",
    "                )\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series number of spec ratings: `get_series_spec_rating_count(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_spec_rating_count(series_soup):\n",
    "    '''\n",
    "    Scrape the series number of spectator ratings from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series number of spectator ratings. None if not found.'''\n",
    "    series_ratings = series_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in series_ratings:\n",
    "        if \"Spectateurs\" in ratings.text:\n",
    "            return float(\n",
    "                re.match(\n",
    "                    r\"\\s\\d+\",\n",
    "                    ratings.find(\"span\", {\"class\": \"stareval-review\"}).text\n",
    "                ).group()  \n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series summary: `get_series_summary(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_summary(series_soup):\n",
    "    '''\n",
    "    Scrape the series summary from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series summary. None if not found.'''\n",
    "    series_summary = series_soup.find(\n",
    "            \"section\", {\"class\": \"section ovw ovw-synopsis\"}\n",
    "        ).find(\"div\", {\"class\": \"content-txt\"})\n",
    "    if series_summary:\n",
    "        series_summary = series_summary.text.strip()\n",
    "        return unicodedata.normalize(\"NFKC\", series_summary)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get series poster: `get_series_poster(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_poster(series_soup):\n",
    "    '''\n",
    "    Scrape the series poster link from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series poster link.'''\n",
    "    # Get the series poster\n",
    "    return series_soup.find(\"img\", {\"class\": \"thumbnail-img\"})[\"src\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download series poster: `download_series_poster(poster_url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_series_poster(poster_link, series_name):\n",
    "    '''\n",
    "    Download the series poster from the series poster link.\n",
    "    :param poster_link: The series poster link.\n",
    "    :param series_name: The series name.\n",
    "    :return: Nothing but saves the series poster as a jpg file.'''\n",
    "    poster = urlopen(poster_link)\n",
    "    poster_path = \"../Series/Posters/\"\n",
    "    os.makedirs(os.path.dirname(poster_path), exist_ok=True) #create folders if not exists\n",
    "    save_path = f\"{poster_path}{series_name.replace(' ','_').replace('_:_','_').replace(':_','_').replace(',','')}_poster.jpg\"\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(poster.read())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `ScrapeURL(series_url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the data from the series urls\n",
    "# The function return a dataframe and a list of url that return error.\n",
    "# And save them into csv files (allocine_series.csv and allocine_errors.csv)\n",
    "def ScrapeURL(series_url: list, dwld_poster: bool = False):\n",
    "    '''\n",
    "    Scrape the data from the series page url.\n",
    "    :param series_url: The list of series page url.\n",
    "    :param dwld_poster: Whether to download the series poster.\n",
    "    :return: The dataframe of the series data and the dataframe of urls that return an error. Both are saved into csv files.'''\n",
    "    \n",
    "    # init the dataframe\n",
    "    c = [\"id\",\n",
    "        \"title\",\n",
    "        \"status\",\n",
    "        \"release_date\",\n",
    "        \"duration\",\n",
    "        \"nb_seasons\",\n",
    "        \"nb_episodes\",\n",
    "        \"genres\",\n",
    "        \"directors\",\n",
    "        \"actors\",\n",
    "        \"nationality\",\n",
    "        \"press_rating\",\n",
    "        \"nb_press_rating\",\n",
    "        \"spec_rating\",\n",
    "        \"nb_spec_rating\",\n",
    "        \"summary\",\n",
    "        \"poster_link\"\n",
    "    ]\n",
    "    df = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    n_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors = []\n",
    "    \n",
    "    # request loop\n",
    "    for url in series_url:\n",
    "        try :\n",
    "            response = urlopen(url)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Monitoring the requests\n",
    "            n_request += 1\n",
    "            \n",
    "            elapsed_time = time() - start_time\n",
    "            print(f'Request #{n_request}; Frequency: {n_request/elapsed_time} requests/s')\n",
    "            clear_output(wait = True)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Warning for non-200 status codes\n",
    "            if response.status != 200:\n",
    "                warn('Request #{}; Status code: {}'.format(n_request, response.status_code))\n",
    "                errors.append(url)\n",
    "\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            html_text = response.read().decode(\"utf-8\")\n",
    "            series_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "            \n",
    "            if series_html_soup.find('div', 'titlebar-title'):\n",
    "                # Scrape the series ID \n",
    "                tp_id = get_series_ID(series_html_soup)\n",
    "                # Scrape the title\n",
    "                tp_title = get_series_title(series_html_soup)\n",
    "                # Scrape the status\n",
    "                tp_status = get_series_status(series_html_soup)\n",
    "                # Scrape the release date\n",
    "                tp_release_dt = get_series_release_date(series_html_soup)\n",
    "                # Scrape the duration\n",
    "                tp_duration = get_series_duration(series_html_soup)\n",
    "                # Scrape the number of seasons and episodes\n",
    "                tp_nb_seasons, tp_nb_episodes = get_series_nb_seasons_episodes(series_html_soup)\n",
    "                # Scrape the genres\n",
    "                tp_genre = get_series_genres(series_html_soup)\n",
    "                # Scrape the directors\n",
    "                tp_director = get_series_directors(series_html_soup)\n",
    "                # Scrape the actors\n",
    "                tp_actor = get_series_actors(series_html_soup)\n",
    "                # Scrape the nationality\n",
    "                tp_nation = get_series_nationality(series_html_soup)\n",
    "                # Scrape the press ratings\n",
    "                tp_press_rating = get_series_press_rating(series_html_soup)\n",
    "                # Scrape the number of press ratings\n",
    "                tp_nb_press_rating = get_series_press_rating_count(series_html_soup)\n",
    "                # Scrape the spec ratings\n",
    "                tp_spec_rating = get_series_spec_rating(series_html_soup)\n",
    "                # Scrape the number of spec ratings\n",
    "                tp_nb_spec_rating = get_series_spec_rating_count(series_html_soup)\n",
    "                # Scrape the summary\n",
    "                tp_summary = get_series_summary(series_html_soup)\n",
    "                # Scrape the poster\n",
    "                tp_poster = get_series_poster(series_html_soup)\n",
    "                # Download the poster (optional)\n",
    "                if dwld_poster:\n",
    "                    download_series_poster(tp_poster, tp_title)\n",
    "                \n",
    "                # Append the data\n",
    "                df_tmp = pd.DataFrame({'id': [tp_id],\n",
    "                                       'title': [tp_title],\n",
    "                                       'status': [tp_status],\n",
    "                                       'release_date': [tp_release_dt],\n",
    "                                       'duration': [tp_duration],\n",
    "                                       'nb_seasons': [tp_nb_seasons],\n",
    "                                       'nb_episodes': [tp_nb_episodes],\n",
    "                                       'genres': [tp_genre],\n",
    "                                       'directors': [tp_director],\n",
    "                                       'actors': [tp_actor],\n",
    "                                       'nationality': [tp_nation],\n",
    "                                       'press_rating': [tp_press_rating],\n",
    "                                       'nb_press_rating': [tp_nb_press_rating],\n",
    "                                       'spec_rating': [tp_spec_rating],\n",
    "                                       'nb_spec_rating': [tp_nb_spec_rating],\n",
    "                                       'summary': [tp_summary],\n",
    "                                       'poster_link': [tp_poster]})\n",
    "                \n",
    "                df = pd.concat([df, df_tmp], ignore_index=True)\n",
    "                \n",
    "        except:\n",
    "            errors.append(url)\n",
    "            warn(f'Request #{n_request} fail; Total errors : {len(errors)}')\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    # monitoring \n",
    "    \n",
    "    series_path = '../Series/Data/'\n",
    "    os.makedirs(os.path.dirname(series_path), exist_ok=True) #create folders if not exists\n",
    "    elapsed_time = time() - start_time\n",
    "    print(f'Done; {n_request} requests in {timedelta(seconds=elapsed_time)} with {len(errors)} errors')\n",
    "    clear_output(wait = True)\n",
    "    df.to_csv(f\"{series_path}allocine_series.csv\", index=False)\n",
    "    # list to dataframe\n",
    "    errors_df = pd.DataFrame(errors, columns=['url'])\n",
    "    errors_df.to_csv(f\"{series_path}allocine_errors.csv\")\n",
    "    # return dataframe and errors\n",
    "    return df, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting series urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 20 Page Requests and 300 Series Requests in 0:00:54.662713\n"
     ]
    }
   ],
   "source": [
    "# Scrape the page from start_page to end_page (included) or with nb_pages\n",
    "start_page = 1\n",
    "end_page = None\n",
    "nb_pages = 20\n",
    "getSeriesUrl(start_page, end_page=end_page, nb_pages=nb_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the list of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of urls \n",
    "m_url = pd.read_csv(\"../Series/Data/series_url.csv\",names=['url'])\n",
    "m_url = m_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done; 300 requests in 0:17:26.159589 with 4 errors\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data \n",
    "d, e = ScrapeURL(m_url)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff1774c64d83fce3825259fa3771bbe70271854497325f5fa1e2c1b92279703"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
