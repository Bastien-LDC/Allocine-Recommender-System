{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎦**Web Scraping Series Data From AlloCiné.fr**📺\n",
    "\n",
    "This script builds a DataFrame by web scraping the **series** data from AlloCiné — a company which provides information on French cinema. Because of the long delay, we choose to scrape the data in two steps : \n",
    "- First we scrape the url of each series with `getSeriesUrl(...)`\n",
    "- Lastly we use the url list to scrape the data for each series with `ScrapeURL(...)`\n",
    "\n",
    "*📝Note 1: We use the popular BeautifulSoup package*\n",
    "\n",
    "## **Functions :**\n",
    "\n",
    "### `getSeriesUrl(start_page, end_page, nb_pages)` :\n",
    "\n",
    "Saves a CSV file of the series urls list as `../Series/Data/series_url.csv`. The argument must be integers and are used to select the range of pages (or the number of pages, default 1) you want to scrape the data from. The `end_page` is included. If the `end_page` value is correct (not `None` or >= `start_page`), the `nb_pages` argument is ignored.\n",
    "\n",
    "### `ScrapeURL(series_url, dwld_poster)` :\n",
    "\n",
    "Iterate over the list of url generated by `getSeriesUrl(...)` and scrape the data for each series. In the process, we extract :\n",
    "\n",
    "- `id` : Allocine series id\n",
    "- `title` : the series title (in French)\n",
    "- `status` : the series status (in French) (En cours|Terminée|Annulée)\n",
    "- `release_date`: the series release date\n",
    "- `duration`: the series average episode length (in minutes)\n",
    "- `nb_seasons`: the number of seasons\n",
    "- `nb_episodes`: the number of episodes\n",
    "- `genres` : the series genres (as a CSV string)\n",
    "- `directors` : series directors (as a CSV string)\n",
    "- `actors` : main actors of the series (as a CSV string)\n",
    "- `nationality`: nationality of the series (as a CSV string)\n",
    "- `press_rating`: average press rating (from 0.5 to 5 stars ⭐⭐⭐⭐⭐)\n",
    "- `nb_press_rating`: number of ratings made by the press\n",
    "- `spect_rating`: average AlloCiné users rating (from 0.5 to 5 stars ⭐⭐⭐⭐⭐)\n",
    "- `nb_spect_rating`: number of ratings made by the users/spectators\n",
    "- `summary`: the series summary\n",
    "- `poster_link`: url of the series poster\n",
    "\n",
    "*📝Note 2: We can choose to download the poster image with the `dwld_poster` argument. If `True`, the poster image is downloaded and saved in the `../Series/Posters/` folder.*\n",
    "\n",
    "*📝Note 3: Intermediate functions were created to retrieve each individual feature of the dataframe, which makes it easy for debugging.*\n",
    "\n",
    "The function `ScrapeURL(...)` returns two objects : the data as a dataframe and the url list of errors as a list. In addition the two objects are saved as `../Series/Data/allocine_movies.csv` and `../Series/Data/allocine_errors.csv`. At this time, the remaining errors are not handled and integrated to the dataframe, because the issues have not been all identified yet.\n",
    "\n",
    "(⚠️ **Warning** ⚠️: the process can take a while, depending on the number of pages you choose to scrape. It is recommended to use a dedicated computer for this process, as it can take a long time to complete.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Import libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\AppData\\Local\\Temp\\ipykernel_26140\\1372620999.py:15: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from time import time\n",
    "from time import sleep\n",
    "from datetime import timedelta\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions:** Getting series infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series ID: `get_series_ID(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_ID(series_soup: BeautifulSoup) -> int:\n",
    "    '''\n",
    "    Scrape the series ID from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series ID.'''\n",
    "    series_ID = int(re.sub(\n",
    "        r\"\\D\", \"\", series_soup.find(\"nav\", {\"class\": \"third-nav\"}).a[\"href\"]\n",
    "        ))\n",
    "    return series_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series title: `get_series_title(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_title(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series title from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series title.'''\n",
    "    series_title = series_soup.find(\"div\", {\"class\": \"titlebar-title\"}).text.strip()\n",
    "    return series_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series status: `get_series_status(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_status(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series status from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series status (En cours|Terminée|Annulée).'''\n",
    "    series_status = series_soup.find(\"div\", {\"class\": \"label-status\"}).text.strip()\n",
    "    return series_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series release date: `get_series_release_date(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_release_date(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series release date from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series release date.'''\n",
    "    series_release_date = series_soup.find(\"div\", {\"class\": \"meta-body-item meta-body-info\"})\n",
    "    if series_release_date:\n",
    "        series_release_date = series_release_date.text.strip().split(\"\\n\")[0]\n",
    "        if get_series_status(series_soup) == \"Terminée\":\n",
    "            series_release_date = series_release_date.replace(\" \", \"\")      \n",
    "    return series_release_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series duration: `get_series_duration(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_duration(series_soup: BeautifulSoup) -> int:\n",
    "    '''\n",
    "    Scrape the series duration from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series duration in minutes.'''\n",
    "    series_duration = series_soup.find(\"span\", {\"class\": \"spacer\"}).next_sibling.strip()\n",
    "    if series_duration != \"\":\n",
    "        duration_timedelta = pd.to_timedelta(series_duration).components\n",
    "        series_duration = duration_timedelta.hours * 60 + duration_timedelta.minutes\n",
    "    return int(series_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series nb of seasons and episodes: `get_series_nb_seasons_episodes(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_nb_seasons_episodes(series_soup: BeautifulSoup) -> tuple[int,int]:\n",
    "    '''\n",
    "    Scrape the series number of seasons and episodes from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series number of seasons and episodes. None if not found.'''\n",
    "    series_nb_seasons_episodes = series_soup.find_all(\"div\", {\"class\": \"stats-numbers-row-item\"})\n",
    "    if series_nb_seasons_episodes:\n",
    "        series_nb_seasons = series_nb_seasons_episodes[0].div.text\n",
    "        series_nb_episodes = series_nb_seasons_episodes[1].div.text\n",
    "        return int(series_nb_seasons), int(series_nb_episodes)\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series genres: `get_series_genres(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_genres(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series genres from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series genres as a CSV string. None if not found.'''\n",
    "    div_genres = series_soup.find(\"div\", {\"class\": \"meta-body-item meta-body-info\"})\n",
    "    if div_genres:\n",
    "        series_genres = [\n",
    "            genre.text\n",
    "            for genre in div_genres.find_all(\"span\")\n",
    "            if \"\\n\" and '/' not in genre.text\n",
    "        ]\n",
    "        return \", \".join(series_genres)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series directors: `get_series_directors(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_directors(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series directors from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series directors as a CSV string. None if not found.'''\n",
    "    div_directors = series_soup.find(\n",
    "            \"div\", {\"class\": \"meta-body-item meta-body-direction\"}\n",
    "        )\n",
    "    if div_directors:\n",
    "        # We retrieve the people next to the \"Créée par\" keyword\n",
    "        series_directors = [\n",
    "            link.text\n",
    "            for link in div_directors.find_all(\n",
    "                [\"a\", \"span\"], class_=re.compile(r\".*blue-link$\")\n",
    "            )\n",
    "        ]\n",
    "        return \", \".join(series_directors)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series actors: `get_series_actors(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_actors(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series actors from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series actors as a CSV string. None if not found.'''\n",
    "    div_actors = series_soup.find(\"div\", {\"class\": \"meta-body-item meta-body-actor\"})\n",
    "    if div_actors:\n",
    "        # We retrieve the people next to the \"Avec\" keyword\n",
    "        series_actors = [actor.text for actor in div_actors.find_all([\"a\", \"span\"])][1:]\n",
    "        return \", \".join(series_actors)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series nationality: `get_series_nationality(series_soup)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_nationality(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series nationality from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series nationality as a CSV string.'''\n",
    "    series_nationality = [\n",
    "            nationality.text.strip()\n",
    "            for nationality in series_soup.find_all(\"span\", class_=\"nationality\")\n",
    "        ]\n",
    "    return \", \".join(series_nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series press ratings: `get_series_press_rating(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_press_rating(series_soup: BeautifulSoup) -> float:\n",
    "    '''\n",
    "    Scrape the series average press rating from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series average press rating. None if not found.'''\n",
    "    series_ratings = series_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in series_ratings:\n",
    "        if \"Presse\" in ratings.text:\n",
    "            # eg: Change from 4,5 to 4.5.\n",
    "            return float(\n",
    "                re.sub(\n",
    "                    \",\", \".\", ratings.find(\"span\", {\"class\": \"stareval-note\"}).text\n",
    "                )\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series number of press ratings: `get_series_press_rating_count(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_press_rating_count(series_soup: BeautifulSoup) -> int:\n",
    "    '''\n",
    "    Scrape the series number of press ratings from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series number of press ratings. None if not found.'''\n",
    "    series_ratings = series_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in series_ratings:\n",
    "        if \"Presse\" in ratings.text:\n",
    "            # We keep only the number of ratings, and we leave the number of reviews out.\n",
    "            # (eg: re.match(\"\\s\\d+\", \" 10154 notes dont 1327 critiques\").group() returns \" 10154\")\n",
    "            return int(\n",
    "                re.match(\n",
    "                    r\"\\s\\d+\",\n",
    "                    ratings.find(\"span\", {\"class\": \"stareval-review\"}).text,\n",
    "                ).group()\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series spectator ratings: `get_series_spec_rating(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_spec_rating(series_soup: BeautifulSoup) -> float:\n",
    "    '''\n",
    "    Scrape the series average spectator rating from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series average spectator rating. None if not found.'''\n",
    "    series_ratings = series_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in series_ratings:\n",
    "        if \"Spectateurs\" in ratings.text:\n",
    "            # eg: Change from 4,5 to 4.5.\n",
    "            return float(\n",
    "                re.sub(\n",
    "                    \",\", \".\", ratings.find(\"span\", {\"class\": \"stareval-note\"}).text\n",
    "                )\n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series number of spec ratings: `get_series_spec_rating_count(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_spec_rating_count(series_soup: BeautifulSoup) -> int:\n",
    "    '''\n",
    "    Scrape the series number of spectator ratings from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series number of spectator ratings. None if not found.'''\n",
    "    series_ratings = series_soup.find_all(\"div\", class_=\"rating-item\")\n",
    "    for ratings in series_ratings:\n",
    "        if \"Spectateurs\" in ratings.text:\n",
    "            # We keep only the number of ratings, and we leave the number of reviews out.\n",
    "            # (eg: re.match(\"\\s\\d+\", \" 10154 notes dont 1327 critiques\").group() returns \" 10154\")\n",
    "            return int(\n",
    "                re.match(\n",
    "                    r\"\\s\\d+\",\n",
    "                    ratings.find(\"span\", {\"class\": \"stareval-review\"}).text\n",
    "                ).group()  \n",
    "            )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series summary: `get_series_summary(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_summary(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series summary from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series summary. None if not found.'''\n",
    "    series_summary = series_soup.find(\n",
    "            \"section\", {\"class\": \"section ovw ovw-synopsis\"}\n",
    "        ).find(\"div\", {\"class\": \"content-txt\"})\n",
    "    if series_summary:\n",
    "        series_summary = series_summary.text.strip()\n",
    "        return unicodedata.normalize(\"NFKC\", series_summary)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get series poster: `get_series_poster(series_soup)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_poster(series_soup: BeautifulSoup) -> str:\n",
    "    '''\n",
    "    Scrape the series poster link from the series page.\n",
    "    :param series_soup: BeautifulSoup object of the series page.\n",
    "    :return: The series poster link.'''\n",
    "    return series_soup.find(\"img\", {\"class\": \"thumbnail-img\"})[\"src\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download series poster: `download_series_poster(poster_url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_series_poster(poster_link, series_name: str) -> None:\n",
    "    '''\n",
    "    Download the series poster from the series poster link.\n",
    "    :param poster_link: The series poster link.\n",
    "    :param series_name: The series name.\n",
    "    :return: Nothing but saves the series poster as a jpg file.'''\n",
    "    poster = urlopen(poster_link)\n",
    "    poster_path = \"../Series/Posters/\"\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(poster_path), exist_ok=True) \n",
    "    save_path = f\"{poster_path}{series_name.replace(' ','_').replace('_:_','_').replace(':_','_').replace(',','')}_poster.jpg\"\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(poster.read())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download all series posters: `download_all_series_posters(series_df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_series_posters(series_df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Download all the series posters from the poster links in the series dataframe.\n",
    "    :param series_df: The series dataframe.\n",
    "    :return: nothing but saves the series posters as jpg files.'''\n",
    "    for index, row in series_df.iterrows():\n",
    "        download_series_poster(row[\"poster_link\"], row[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Function:** `getSeriesUrl(start_page, end_page, nb_pages)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeriesUrl(start_page: int, end_page: int=None, nb_pages: int=1) -> None:\n",
    "    '''\n",
    "    Scrape the series urls from the AlloCine website's series page (http://www.allocine.fr/series-tv/).\n",
    "    The range of pages to scrape goes from start_page to end_page (included) if end_page is not None or <= start_page.\n",
    "    Else, the range of pages to scrape goes from start_page to start_page + nb_pages.\n",
    "    It will ignore the series that has not been released yet.\n",
    "    :param start_page: The first page to scrape.\n",
    "    :param end_page: The last page to scrape (included) (optional).\n",
    "    :param nb_pages: The number of pages to scrape (default 1).\n",
    "    :return:  Nothing but saves the list of series urls in a csv file.'''\n",
    "    if start_page <= 0:\n",
    "        raise ValueError('start_page must be positive !')\n",
    "\n",
    "    # Set the list\n",
    "    series_url = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = start_page\n",
    "    # We will scrape by default at least 1 page if end_page is not specified, \n",
    "    # or if it is lower than start_page,\n",
    "    # or if the number of pages to scrape is negative.\n",
    "    if nb_pages < 1:\n",
    "        nb_pages = 1\n",
    "    if end_page == None or end_page < start_page:\n",
    "        end_page = start_page + nb_pages - 1\n",
    "    \n",
    "    # Number of series requests\n",
    "    s_requests = 0\n",
    "        \n",
    "    # Loop over the pages\n",
    "    for p in range(start_page, end_page + 1):\n",
    "\n",
    "        # Get request\n",
    "        url = f'https://www.allocine.fr/series-tv/?page={p}'\n",
    "        response = urlopen(url)\n",
    "        \n",
    "        # Pause the loop\n",
    "        sleep(randint(1,2))\n",
    "            \n",
    "        # Monitoring the requests\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'>Page Request: {p_requests}; Frequency: {p_requests/elapsed_time} requests/s')\n",
    "        clear_output(wait = True)\n",
    "            \n",
    "        # Warning for non-200 status codes\n",
    "        if response.status != 200:\n",
    "            warn(f'>Page Request: {p_requests}; Status code: {response.status_code}')\n",
    "\n",
    "        # Break the loop if the number of requests is greater than expected\n",
    "        if p_requests > end_page:\n",
    "            warn('Number of requests was greater than expected.')\n",
    "            break\n",
    "\n",
    "        # Parse the content of the request with BeautifulSoup\n",
    "        html_text = response.read().decode(\"utf-8\")\n",
    "        html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "        # Select all the series url from a single page\n",
    "        series = html_soup.find_all('h2', 'meta-title')\n",
    "        s_requests += len(series)\n",
    "        # Count the number of series not yet released \n",
    "        nr_series = 0\n",
    "       \n",
    "        # Monitoring the requests\n",
    "        print(f'>Page Request: {p_requests}; Series Request: {s_requests}')\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        # Pause the loop\n",
    "        sleep(1)\n",
    "        \n",
    "        for serie in series:\n",
    "            s_url = f'http://www.allocine.fr{serie.a[\"href\"]}'\n",
    "            s_soup = BeautifulSoup(urlopen(s_url), 'html.parser')\n",
    "            s_status = get_series_status(s_soup)\n",
    "            # We keep the movie url only if the series has already been released.\n",
    "            if s_status != \"À venir\":\n",
    "                series_url.append(s_url)\n",
    "            else:\n",
    "                nr_series += 1              \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    series_path = '../Series/Data/'\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(series_path), exist_ok=True)\n",
    "    print(f'--> Done; {p_requests-start_page} Page Requests and {s_requests-nr_series}/{s_requests} Series Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    r = np.asarray(series_url)\n",
    "    np.savetxt(f\"{series_path}series_url.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function:** `ScrapeURL(series_url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeURL(series_url: list, dwld_poster: bool = False) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Scrape the data from the series page url.\n",
    "    :param series_url: The list of series page url.\n",
    "    :param dwld_poster: Boolean to download the series poster (default False).\n",
    "    :return: The dataframe of the series data and the dataframe of urls that return an error. \n",
    "    Both are saved into csv files.'''    \n",
    "    # init the dataframe\n",
    "    c = [\"id\",\n",
    "        \"title\",\n",
    "        \"status\",\n",
    "        \"release_date\",\n",
    "        \"duration\",\n",
    "        \"nb_seasons\",\n",
    "        \"nb_episodes\",\n",
    "        \"genres\",\n",
    "        \"directors\",\n",
    "        \"actors\",\n",
    "        \"nationality\",\n",
    "        \"press_rating\",\n",
    "        \"nb_press_rating\",\n",
    "        \"spect_rating\",\n",
    "        \"nb_spect_rating\",\n",
    "        \"summary\",\n",
    "        \"poster_link\"\n",
    "    ]\n",
    "    df = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    n_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors = []\n",
    "    \n",
    "    # request loop\n",
    "    for url in series_url:\n",
    "        try :\n",
    "            response = urlopen(url)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Monitoring the requests\n",
    "            n_request += 1\n",
    "            \n",
    "            elapsed_time = time() - start_time\n",
    "            print(f'Request #{n_request}; Frequency: {n_request/elapsed_time} requests/s')\n",
    "            clear_output(wait = True)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Warning for non-200 status codes\n",
    "            if response.status != 200:\n",
    "                warn('Request #{}; Status code: {}'.format(n_request, response.status_code))\n",
    "                errors.append(url)\n",
    "\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            html_text = response.read().decode(\"utf-8\")\n",
    "            series_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "            \n",
    "            if series_html_soup.find('div', 'titlebar-title'):\n",
    "                # Scrape the series ID \n",
    "                tp_id = get_series_ID(series_html_soup)\n",
    "                # Scrape the title\n",
    "                tp_title = get_series_title(series_html_soup)\n",
    "                # Scrape the status\n",
    "                tp_status = get_series_status(series_html_soup)\n",
    "                # Scrape the release date\n",
    "                tp_release_dt = get_series_release_date(series_html_soup)\n",
    "                # Scrape the duration\n",
    "                tp_duration = get_series_duration(series_html_soup)\n",
    "                # Scrape the number of seasons and episodes\n",
    "                tp_nb_seasons, tp_nb_episodes = get_series_nb_seasons_episodes(series_html_soup)\n",
    "                # Scrape the genres\n",
    "                tp_genre = get_series_genres(series_html_soup)\n",
    "                # Scrape the directors\n",
    "                tp_director = get_series_directors(series_html_soup)\n",
    "                # Scrape the actors\n",
    "                tp_actor = get_series_actors(series_html_soup)\n",
    "                # Scrape the nationality\n",
    "                tp_nation = get_series_nationality(series_html_soup)\n",
    "                # Scrape the press ratings\n",
    "                tp_press_rating = get_series_press_rating(series_html_soup)\n",
    "                # Scrape the number of press ratings\n",
    "                tp_nb_press_rating = get_series_press_rating_count(series_html_soup)\n",
    "                # Scrape the spec ratings\n",
    "                tp_spec_rating = get_series_spec_rating(series_html_soup)\n",
    "                # Scrape the number of spec ratings\n",
    "                tp_nb_spec_rating = get_series_spec_rating_count(series_html_soup)\n",
    "                # Scrape the summary\n",
    "                tp_summary = get_series_summary(series_html_soup)\n",
    "                # Scrape the poster\n",
    "                tp_poster = get_series_poster(series_html_soup)\n",
    "                # Download the poster (optional)\n",
    "                if dwld_poster:\n",
    "                    download_series_poster(tp_poster, tp_title)\n",
    "                \n",
    "                # Append the data\n",
    "                df_tmp = pd.DataFrame({'id': [tp_id],\n",
    "                                       'title': [tp_title],\n",
    "                                       'status': [tp_status],\n",
    "                                       'release_date': [tp_release_dt],\n",
    "                                       'duration': [tp_duration],\n",
    "                                       'nb_seasons': [tp_nb_seasons],\n",
    "                                       'nb_episodes': [tp_nb_episodes],\n",
    "                                       'genres': [tp_genre],\n",
    "                                       'directors': [tp_director],\n",
    "                                       'actors': [tp_actor],\n",
    "                                       'nationality': [tp_nation],\n",
    "                                       'press_rating': [tp_press_rating],\n",
    "                                       'nb_press_rating': [tp_nb_press_rating],\n",
    "                                       'spect_rating': [tp_spec_rating],\n",
    "                                       'nb_spect_rating': [tp_nb_spec_rating],\n",
    "                                       'summary': [tp_summary],\n",
    "                                       'poster_link': [tp_poster]})\n",
    "                \n",
    "                df = pd.concat([df, df_tmp], ignore_index=True)                \n",
    "        except:\n",
    "            errors.append(url)\n",
    "            warn(f'Request #{n_request} fail; Total errors : {len(errors)}')\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    # monitoring     \n",
    "    series_path = '../Series/Data/'\n",
    "    # We create the folder if not exists\n",
    "    os.makedirs(os.path.dirname(series_path), exist_ok=True)\n",
    "    elapsed_time = time() - start_time\n",
    "    print(f'Done; {n_request} requests in {timedelta(seconds=elapsed_time)} with {len(errors)} errors')\n",
    "    clear_output(wait = True)\n",
    "    df.to_csv(f\"{series_path}allocine_series.csv\", index=False)\n",
    "    # list to dataframe\n",
    "    errors_df = pd.DataFrame(errors, columns=['url'])\n",
    "    errors_df.to_csv(f\"{series_path}allocine_errors.csv\")\n",
    "    # return dataframe and errors\n",
    "    return df, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Launching the script**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting series urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 1 Page Requests and 15/15 Series Requests in 0:00:07.512769\n"
     ]
    }
   ],
   "source": [
    "# Scrape the page from start_page to end_page (included) or with nb_pages\n",
    "start_page = 1\n",
    "end_page = None\n",
    "nb_pages = 1\n",
    "getSeriesUrl(start_page, end_page=end_page, nb_pages=nb_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the list of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of urls \n",
    "s_url = pd.read_csv(\"../Series/Data/series_url.csv\",names=['url'])\n",
    "s_url = s_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done; 15 requests in 0:00:50.214055 with 0 errors\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data \n",
    "series_df, series_errors = ScrapeURL(s_url)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff1774c64d83fce3825259fa3771bbe70271854497325f5fa1e2c1b92279703"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
