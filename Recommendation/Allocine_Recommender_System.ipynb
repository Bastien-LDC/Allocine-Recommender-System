{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¶**AlloCin√© Recommender System**üîéüìç\n",
    "\n",
    "Once we cleaned the data, we can start to build our recommender system. The data that will be used is located in the `../Cleaned Data/` folder.\n",
    "\n",
    "**Types of Recommender System:**\n",
    "\n",
    "There are two types of recommender system: **`content-based`** and **`collaborative-filtering`**.\n",
    "\n",
    "- **`Content-based`:** this recommender system is based only on the characteristics of the products. Here, we will recommend an item to a user by comparing the features between items and recommend the items with the highest similarity.\n",
    "\n",
    "- **`Collaborative-filtering`:** this recommender system is based on the interactions between users and the items. \n",
    "    - `Memory-based`: Calculation of similarities between users or between products to recommend the right product.\n",
    "        - `User-based CF`: Suggest products that have been well rated by similar users.\n",
    "        - `Item-based CF`: Offer products that are well rated and similar to products already used.\n",
    "    - `Model-based`: Finding a pattern explaining browsing and preferences behaviors.\n",
    "\n",
    "By the format of our data, we will only be able to perform an `item-based CF` for the `memory-based` method as we don't have any information about the users, only their ratings and the features of the items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Import libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from ast import literal_eval\n",
    "from warnings import filterwarnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop # Used to get the French stop-words\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# We ignore dateparse warnings\n",
    "filterwarnings(\"ignore\",message=\"The localize method is no longer necessary, as this time zone supports the fold attribute\")\n",
    "# We ignore reindexing warnings\n",
    "filterwarnings(\"ignore\",message=\"Boolean Series key will be reindexed\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    '''\n",
    "    Load the csv files and return a dict of dataframes.\n",
    "    '''\n",
    "    root_path = f\"../Cleaned Data/\"\n",
    "    movies = pd.read_csv(f\"{root_path}movies.csv\", converters={\"genres\": literal_eval}) # Load list look-alike string as type list\n",
    "    series = pd.read_csv(f\"{root_path}series.csv\", converters={\"genres\": literal_eval})\n",
    "    for c in [\"actors\", \"directors\", \"nationality\"]:\n",
    "        movies[c] = movies[c].apply(lambda x: literal_eval(x) if type(x) == str else [])\n",
    "        series[c] = series[c].apply(lambda x: literal_eval(x) if type(x) == str else [])\n",
    "    press_movies = pd.read_csv(f\"{root_path}press_movies.csv\")\n",
    "    press_series = pd.read_csv(f\"{root_path}press_series.csv\")\n",
    "    user_movies = pd.read_csv(f\"{root_path}user_movies.csv\")\n",
    "    user_series = pd.read_csv(f\"{root_path}user_series.csv\")\n",
    "    #user_series = pd.read_csv(f\"../Series/Ratings/Webscraping_Series_Ratings_user_ratings_series_#1-1.csv\")\n",
    "    return {\"movies\":movies, \"series\":series, \"press_movies\":press_movies, \"press_series\":press_series, \"user_movies\":user_movies, \"user_series\":user_series}\n",
    "data = load_csv()\n",
    "movies, series, press_movies, press_series, user_movies, user_series = data[\"movies\"], data[\"series\"], data[\"press_movies\"], data[\"press_series\"], data[\"user_movies\"], data[\"user_series\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Content-based (CB)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    '''\n",
    "    Get the French stop-words for tagging.\n",
    "    :return: list of French stop-words.\n",
    "    '''\n",
    "    # turn French stop-words into a list\n",
    "    stop_words = list(fr_stop)\n",
    "    # Load extra stop-words in French and in English\n",
    "    with open(\"stop_words_french.txt\",'r', encoding='UTF-8') as file:\n",
    "        additional_stopwords = file.readlines()\n",
    "        additional_stopwords = [line.rstrip() for line in additional_stopwords]\n",
    "    with open(\"stop_words_eng.txt\",'r', encoding='UTF-8') as file:\n",
    "        additional_stopwords_eng = file.readlines()\n",
    "        additional_stopwords_eng = [line.rstrip() for line in additional_stopwords_eng]\n",
    "    \n",
    "    # Add the additional French stop-words to the list of stop-words if they are not already in it\n",
    "    for word, word_eng in zip(additional_stopwords,additional_stopwords_eng):\n",
    "        if word not in stop_words:\n",
    "            stop_words.append(word)\n",
    "        if word_eng not in stop_words:\n",
    "            stop_words.append(word_eng)\n",
    "    stop_words.extend([\"\",\" \",\"#\",\"-\",\":\",\"(\",\")\"])\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(df: pd.DataFrame=None, stop_words: list=get_stop_words(), cols_to_get: str=\"title\"):\n",
    "    '''\n",
    "    Get the tags of the movies from the title and/or summary.\n",
    "    :param df: Dataframe to transform.\n",
    "    :param stop_words: List of stop-words to remove from the tags.\n",
    "    :param col_to_get: Columns to get the tags from.\n",
    "    '''\n",
    "    try:\n",
    "        # We remove unecessary punctuation and characters from the title \n",
    "        # And store the title's keywords in a list\n",
    "        rmv_char = r\"(!|#|:|\\$|\\%|\\^|\\&|\\*|\\(|\\)|-|\\+|/|\\?|\\.+|\\d)\"\n",
    "        df[\"tags\"] = df[cols_to_get].apply(lambda x: re.split(\" |,|\\. |\\.\\.\\.|\\\"|'|-\", re.sub(rmv_char, \" \", x)))\n",
    "        df[\"tags\"] = df[\"tags\"].apply(lambda x: [i for i in x if i.lower() not in stop_words])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies_df(df: pd.DataFrame=None, keep_cols: list=None, cols_to_dummies: list=[\"genres\"]):\n",
    "    '''\n",
    "    Get a dataframe with the dummies of the column cols_to_dummies.\n",
    "    :param df: Dataframe to transform.\n",
    "    :param keep_cols: List of columns to keep.\n",
    "    :param cols_to_dummies: List of columns to transform to dummies.\n",
    "    '''\n",
    "    try:\n",
    "        df_dummies = df[keep_cols]\n",
    "        # We drop the movies with no user rating\n",
    "        df_dummies = df_dummies.dropna(subset=[\"user_rating\"], axis=0)\n",
    "        df_dummies = df_dummies.reset_index(drop=True)\n",
    "        \n",
    "        # We add the tag column to the dataframe\n",
    "        if \"tags\" in cols_to_dummies:\n",
    "            df_dummies = get_tags(df=df_dummies) \n",
    "\n",
    "        non_dummy_cols = df_dummies.shape[1]\n",
    "\n",
    "        # We create binary variables for each cols_to_dummies feature by One-Hot encoding their values\n",
    "        for col in cols_to_dummies:  \n",
    "            encode_col = pd.get_dummies(df_dummies[col].apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "            df_dummies = pd.concat([df_dummies, encode_col], axis=1)\n",
    "        \n",
    "        df_dummies.sort_values(by=[\"title\", \"release_date\"], ascending=[True,False]).reset_index(drop=True, inplace=True)                   \n",
    "        \n",
    "        # We replace NaN values in the dummy columns with 0\n",
    "        df_dummies.iloc[:,non_dummy_cols:] = df_dummies.iloc[:,non_dummy_cols:].fillna(0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    return df_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model CB N¬∞1:** Genres with rating ponderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve only the useful columns for the content-based recommender system\n",
    "cols_to_keep = [\"id\", \"title\", \"release_date\", \"user_rating\", \"genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dummies = get_dummies_df(df=movies, keep_cols=cols_to_keep, cols_to_dummies=[\"genres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the cosine similarity between each movie and multiply it by the user rating.\n",
    "cos_sim = cosine_similarity(movies_dummies.iloc[:,-(movies_dummies.shape[1]-len(cols_to_keep)):]) * movies_dummies.user_rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CB_recommender(title: str=\"The Batman\", cos_sim=cos_sim, nb_recos: int=10, dummies_df: pd.DataFrame=movies_dummies):\n",
    "    '''\n",
    "    Get the recommendations for a movie.\n",
    "    :param title: Title of the movie to get the recommendations for.\n",
    "    :param cos_sim: Cosine similarity matrix.\n",
    "    :param nb_recos: Number of recommendations to get.\n",
    "    :param dummies_df: Dataframe with the dummies of the movies.\n",
    "    :return: List of nb_recos recommendations.    \n",
    "    '''\n",
    "    CB_recos = [] # Initialisation of the list of recommendations\n",
    "    title_keywords = title.split(\" \") # We split the title into keywords\n",
    "    title_list = []\n",
    "    # We collect all the movies titles which contain all the keywords of the title\n",
    "    for full_title in dummies_df.title.values.tolist():\n",
    "        if all(word.lower() in full_title.lower() for word in title_keywords):\n",
    "            title_list.append(full_title)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    # If we get a direct match, we return the title.    \n",
    "    if len(title_list) == 1:\n",
    "        current_title = title_list[0]\n",
    "    # Else, we get all the movies with similar names and ask the user to choose one.\n",
    "    elif len(title_list) > 1:\n",
    "        print(\"Several movies found with similar title. Please choose one of the following:\")\n",
    "        for i, item in enumerate(title_list,1):\n",
    "            print(i, ': ' + item, sep='',end='\\n')\n",
    "        choice = -1\n",
    "        while choice < 1 or choice > len(title_list):\n",
    "            choice = int(input(\"Enter the number of the movie: \"))\n",
    "        current_title = title_list[choice-1]\n",
    "    else:\n",
    "        return f'Error: The movie {title} requested was not found.'\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"The movie you requested is: \\\"{current_title}\\\"\\nIf you like this movie, you might also like:\")\n",
    "    # If the movie is in the database\n",
    "    idx = dummies_df.index[dummies_df.title == current_title][0] # We retrieve the index of the movie (the most recent one if there are homonyms)\n",
    "    score_series = pd.Series(cos_sim[idx]).drop(idx).sort_values(ascending=False) # We sort the similarity matrix of the movies and we drop the movie itself\n",
    "    top_nbrecos = list(score_series.iloc[0:nb_recos].index) # We select the top nb_recos movies \n",
    "    # We store the titles of the top nb_recos movies in the list CB_recos and return it\n",
    "    for i in top_nbrecos:             \n",
    "        CB_recos.append((dummies_df.title[i], cos_sim[idx][i]))\n",
    "    return CB_recos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie you requested is: \"House of Gucci\"\n",
      "If you like this movie, you might also like:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Green Book : Sur les routes du sud', 4.499999999999999),\n",
       " ('Lion', 4.3999999999999995),\n",
       " ('Bohemian Rhapsody', 4.3999999999999995),\n",
       " ('Elephant Man', 4.3999999999999995),\n",
       " ('Le Mans 66', 4.299999999999999),\n",
       " ('Dallas Buyers Club', 4.299999999999999),\n",
       " ('Bobby : seul contre tous', 4.299999999999999),\n",
       " ('Raging Bull', 4.299999999999999),\n",
       " ('The Great Debaters', 4.299999999999999),\n",
       " ('Rush', 4.299999999999999)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_recommender(title=input(\"Enter a movie title or keywords: \"), cos_sim=cos_sim, nb_recos=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity between movies is very high, so a lot of movies with the highest similarity are recommended to the user. But depending on the original order of the movies, the order of the recommendations may change and therefore may not be very relevant. This is because we only considered the genres as a comparison criterion. We need to use more features or add a ponderation if we want to have a more accurate recommendation. \n",
    "\n",
    "So far, I obtained better recommendation by multiplying the correlation matrix by the movies' ratings. In that way, movies with the same genres will more likely be similar if they have a high rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model CB N¬∞2:** Model 1 + tag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve only the useful columns for the content-based recommender system\n",
    "cols_to_keep_2 = [\"title\", \"release_date\", \"user_rating\", \"genres\", \"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dummies_2 = get_dummies_df(df=movies, keep_cols=cols_to_keep_2, cols_to_dummies=[\"genres\", \"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the cosine similarity between each movie and multiply it by the user rating.\n",
    "cos_sim_2 = cosine_similarity(movies_dummies_2.iloc[:,-(movies_dummies_2.shape[1]-len(cols_to_keep_2) - 1):]) * movies_dummies_2.user_rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie you requested is: \"House of Gucci\"\n",
      "If you like this movie, you might also like:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('42', 2.8991378028648445),\n",
       " ('Lion', 2.5403411844343537),\n",
       " ('Elephant Man', 2.5403411844343537),\n",
       " ('The Great Debaters', 2.482606157515391),\n",
       " ('Bobby : seul contre tous', 2.482606157515391),\n",
       " ('Rush', 2.482606157515391),\n",
       " ('Le Mans 66', 2.482606157515391),\n",
       " ('Le Majordome', 2.424871130596429),\n",
       " ('Braveheart', 2.424871130596429),\n",
       " ('The Blind Side', 2.3671361036774656)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_recommender(title=input(\"Enter a movie title or keywords: \"), cos_sim=cos_sim_2, nb_recos=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second model, we used the titles of the movies to help recommend movies that, at first sight, may seem to talk about the same subject. What we notice from this new model is that movies with that share parts of their title are more likely to be recommended to the user, even though they have a lower rating. Moreover, as we decided to remove all punctuation marks and symbols characters, movies like `\"X-Men\"` will become `\"X Men\"`, and as both words are considered as stop-words, they will never be used to suggests on purpose other movies from the `\"X-Men Saga\"`. Movies with titles like `\"Le Monde de Narnia : Chapitre 1 - Le lion, la sorci√®re blanche et l'armoire magique\"` won't be affected by this characters removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model CB N¬∞3:** Model 1 + actors + directors + nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve only the useful columns for the content-based recommender system\n",
    "cols_to_keep_3 = [\"title\", \"release_date\", \"user_rating\", \"genres\", \"actors\", \"directors\", \"nationality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dummies_3 = get_dummies_df(df=movies, keep_cols=cols_to_keep_3, cols_to_dummies=[\"genres\", \"actors\", \"directors\", \"nationality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the cosine similarity between each movie and multiply it by the user rating.\n",
    "cos_sim_3 = cosine_similarity(movies_dummies_3.iloc[:,-(movies_dummies_3.shape[1]-len(cols_to_keep_3) - 1):]) * movies_dummies_3.user_rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie you requested is: \"Star Wars : Episode III - La Revanche des Sith\"\n",
      "If you like this movie, you might also like:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"Star Wars : Episode II - L'Attaque des clones\", 3.4394767043839667),\n",
       " ('Star Wars : Episode I - La Menace fant√¥me', 3.0467781578016373),\n",
       " ('Star Wars : Episode IV - Un nouvel espoir (La Guerre des √©toiles)',\n",
       "  2.514285714285714),\n",
       " ('Star Wars : Episode VI - Le Retour du Jedi', 2.217391574987466),\n",
       " ('Rogue One: A Star Wars Story', 2.066205785783775),\n",
       " ('Avatar', 1.8428571428571425),\n",
       " ('Star Wars - Le R√©veil de la Force', 1.8142294704442905),\n",
       " ('Retour vers le futur', 1.8040133829088645),\n",
       " ('Indiana Jones et la Derni√®re Croisade', 1.763924196622001),\n",
       " ('Jurassic Park', 1.723835010335137)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_recommender(title=input(\"Enter a movie title or keywords: \"), cos_sim=cos_sim_3, nb_recos=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Collaborative-Filtering (CF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model CF N¬∞1:** *Memory-Based CF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the user_movies df, remove users who appears less than 5 times (=> have made less than 5 reviews)\n",
    "user_movies = user_movies.groupby('user_id').filter(lambda x: len(x) >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movie_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>12</th>\n",
       "      <th>17</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>27</th>\n",
       "      <th>29</th>\n",
       "      <th>...</th>\n",
       "      <th>302227</th>\n",
       "      <th>302530</th>\n",
       "      <th>302598</th>\n",
       "      <th>302673</th>\n",
       "      <th>302792</th>\n",
       "      <th>302942</th>\n",
       "      <th>303281</th>\n",
       "      <th>303310</th>\n",
       "      <th>303312</th>\n",
       "      <th>303478</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.102564</td>\n",
       "      <td>2.983871</td>\n",
       "      <td>3.740741</td>\n",
       "      <td>3.923077</td>\n",
       "      <td>3.655556</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>3.291667</td>\n",
       "      <td>3.060000</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.083242</td>\n",
       "      <td>1.106676</td>\n",
       "      <td>0.801085</td>\n",
       "      <td>1.016782</td>\n",
       "      <td>0.928233</td>\n",
       "      <td>1.217894</td>\n",
       "      <td>1.117187</td>\n",
       "      <td>0.945667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.460361</td>\n",
       "      <td>...</td>\n",
       "      <td>1.089725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.172604</td>\n",
       "      <td>0.763763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>2.474874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 7746 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movie_id     1          2          3          4          12         17      \\\n",
       "count     39.000000  31.000000  27.000000  26.000000  45.000000  50.000000   \n",
       "mean       3.102564   2.983871   3.740741   3.923077   3.655556   3.580000   \n",
       "std        1.083242   1.106676   0.801085   1.016782   0.928233   1.217894   \n",
       "min        0.500000   0.500000   1.500000   1.500000   2.000000   0.500000   \n",
       "25%        2.500000   2.500000   3.500000   3.500000   3.000000   2.625000   \n",
       "50%        3.000000   3.000000   4.000000   4.000000   3.500000   4.000000   \n",
       "75%        4.000000   3.500000   4.000000   5.000000   4.500000   4.500000   \n",
       "max        5.000000   5.000000   5.000000   5.000000   5.000000   5.000000   \n",
       "\n",
       "movie_id     23         24      27         29      ...    302227  302530  \\\n",
       "count     12.000000  50.000000    3.00  50.000000  ...  9.000000     1.0   \n",
       "mean       3.291667   3.060000    3.50   3.000000  ...  2.166667     3.0   \n",
       "std        1.117187   0.945667    0.50   1.460361  ...  1.089725     NaN   \n",
       "min        1.500000   0.500000    3.00   0.500000  ...  1.000000     3.0   \n",
       "25%        2.000000   2.500000    3.25   1.625000  ...  1.500000     3.0   \n",
       "50%        3.750000   3.000000    3.50   3.250000  ...  2.000000     3.0   \n",
       "75%        4.000000   3.500000    3.75   4.000000  ...  3.000000     3.0   \n",
       "max        5.000000   5.000000    4.00   5.000000  ...  4.000000     3.0   \n",
       "\n",
       "movie_id    302598    302673  302792    302942  303281  303310    303312  \\\n",
       "count     6.000000  3.000000     1.0  3.000000     1.0     1.0  2.000000   \n",
       "mean      3.250000  3.833333     0.5  2.833333     3.5     2.5  2.750000   \n",
       "std       1.172604  0.763763     NaN  0.288675     NaN     NaN  0.353553   \n",
       "min       2.000000  3.000000     0.5  2.500000     3.5     2.5  2.500000   \n",
       "25%       2.250000  3.500000     0.5  2.750000     3.5     2.5  2.625000   \n",
       "50%       3.250000  4.000000     0.5  3.000000     3.5     2.5  2.750000   \n",
       "75%       3.875000  4.250000     0.5  3.000000     3.5     2.5  2.875000   \n",
       "max       5.000000  4.500000     0.5  3.000000     3.5     2.5  3.000000   \n",
       "\n",
       "movie_id    303478  \n",
       "count     2.000000  \n",
       "mean      2.250000  \n",
       "std       2.474874  \n",
       "min       0.500000  \n",
       "25%       1.375000  \n",
       "50%       2.250000  \n",
       "75%       3.125000  \n",
       "max       4.000000  \n",
       "\n",
       "[8 rows x 7746 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot table to get the user-movie utility matrix\n",
    "interactions = user_movies.pivot(index='user_id', columns='movie_id', values='user_rating') \n",
    "interactions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We standardize the ratings before filling the missing values \n",
    "magnitude = np.sqrt(np.square(interactions).sum(axis=1))\n",
    "interactions = interactions.divide(magnitude, axis='index')\n",
    "interactions = interactions.fillna(0)\n",
    "\n",
    "user_similarity = 1-pairwise_distances(interactions, metric='cosine')\n",
    "item_similarity = 1-pairwise_distances(interactions.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1).values.reshape(-1, 1)\n",
    "        ratings_diff = (ratings - mean_user_rating)\n",
    "        pred = mean_user_rating + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prediction = predict(interactions, user_similarity, type='user')\n",
    "item_prediction = predict(interactions, item_similarity, type='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_users = list(interactions.index.values)\n",
    "index_movie_id = user_movies.movie_id.unique()\n",
    "index_movies_title = movies[['id','title']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MB_recommendations(user, prediction, nb_recos=10, index_users = index_users, index_movie_id = index_movie_id, index_movies_title = index_movies_title):\n",
    "    # Similarity matrix\n",
    "    user_similarity_df = pd.DataFrame(data=prediction, index=index_users, columns=index_movie_id)\n",
    "    MB_recos = [] # Initialisation of the list of recommendations\n",
    "    # If user in the database\n",
    "    if user in user_similarity_df.index.values:\n",
    "        score_series = pd.Series(user_similarity_df.loc[user]).sort_values(ascending=False) # We sort the similarity matrix\n",
    "        top_nbrecos = list(score_series.iloc[0:nb_recos].index) # We select the most similar nb_recos movies to the movies the user has already seen\n",
    "        # We retrieve the titles of the top nb_recos movies in the list MB_recos and return it\n",
    "        for i in top_nbrecos:\n",
    "            MB_recos.append(index_movies_title[index_movies_title.id == i].title.iloc[0])\n",
    "    else:\n",
    "        print(f'The required user \"{user}\" is not in the database.')\n",
    "    return MB_recos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Festen',\n",
       " 'Du mou dans la g√¢chette',\n",
       " 'Les V√©tos',\n",
       " 'Une affaire de d√©tails',\n",
       " 'Inglourious Basterds',\n",
       " 'Ast√©rix et Ob√©lix : Mission Cl√©op√¢tre',\n",
       " 'Onoda - 10 000 nuits dans la jungle',\n",
       " 'La Route',\n",
       " 'Madeleine Collins',\n",
       " \"Peau d'√¢ne\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_recommendations(\"Z20110828082536017359313\",user_prediction,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assurance sur la mort',\n",
       " 'Bonnie and Clyde',\n",
       " \"L'Homme qui voulut √™tre roi\",\n",
       " 'Stand by Me',\n",
       " 'Le Souffle au coeur',\n",
       " 'Le Flingueur',\n",
       " 'Le Septi√®me Sceau',\n",
       " 'Accattone',\n",
       " \"L'Homme qui r√©tr√©cit\",\n",
       " 'Ben-Hur']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB_recommendations(\"Z20110828082536017359313\",item_prediction,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Z20200307184114217262101    5766\n",
       "Z20051202210829790363634    3554\n",
       "Z20030916115310953701324    3397\n",
       "Z20071016041219617269137    3320\n",
       "Z20110828082536017359313    3076\n",
       "                            ... \n",
       "Z20111105230231077864514       5\n",
       "Z20081110155902317351288       5\n",
       "Z20051129215314737543368       5\n",
       "Z20041129114735730487345       5\n",
       "Z20100520210355010436628       5\n",
       "Name: user_id, Length: 2585, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movies.user_id.value_counts()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48c0ee024c6cc2421e377eb4076cadf4ce6eb7a4584917e3645e162e32894617"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
